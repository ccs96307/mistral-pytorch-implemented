{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, OrderedDict, Optional, List, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "\n",
    "import regex as re\n",
    "import torch\n",
    "from transformers import MistralConfig, MistralForCausalLM, LlamaTokenizer\n",
    "from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformers.utils.hub import cached_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/Tools/anaconda3/envs/llms/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"echarlaix/tiny-random-mistral\"\n",
    "\n",
    "config = MistralConfig.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "model = MistralForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 32,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 37,\n",
       "  \"is_decoder\": true,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"type_vocab_size\": 16,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MistralAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MISTRAL_ATTENTION_CLASSES \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mMistralAttention\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: MistralAttention2,\n\u001b[1;32m      4\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MistralAttention' is not defined"
     ]
    }
   ],
   "source": [
    "MISTRAL_ATTENTION_CLASSES = {\n",
    "    \"eager\": MistralAttention,\n",
    "    \"flash_attention_2\": MistralFlashAttention2,\n",
    "    \"sdpa\": MistralSdpaAttention\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(\n",
    "    key_states: torch.Tensor,\n",
    "    value_states: torch.Tensor,\n",
    "    num_kv_groups: int,\n",
    ") -> torch.Tensor:\n",
    "    batch_size, num_heads, seq_len, head_size = key_states.size()\n",
    "    key_states = key_states[:, :, None, :, :].expand(\n",
    "        batch_size,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        seq_len,\n",
    "        head_size,\n",
    "    )\n",
    "\n",
    "    value_states = value_states[:, :, None, :, :].expand(\n",
    "        batch_size,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        seq_len,\n",
    "        head_size,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        key_states.reshape(batch_size, num_heads * num_kv_groups, seq_len, head_size),\n",
    "        value_states.reshape(batch_size, num_heads * num_kv_groups, seq_len, head_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    \"\"\"Base, abstract class for all caches.\"\"\"\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return the updated key and value states.\"\"\"\n",
    "        raise NotImplementedError(\"Make sure to implement `update` method in a subclass.\")\n",
    "    \n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        raise NotImplementedError(\"Make sure to implement `get_seq_length` in subclass.\")\n",
    "    \n",
    "    def get_max_length(self) -> int:\n",
    "        raise NotImplementedError(\"Make sure to implement `get_max_length` in subclass.\")\n",
    "    \n",
    "    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n",
    "        max_length = self.get_max_length()\n",
    "        previous_seq_length = self.get_seq_length(layer_idx=layer_idx)\n",
    "\n",
    "        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n",
    "            return max_length - new_seq_length\n",
    "        \n",
    "        return previous_seq_length\n",
    "    \n",
    "\n",
    "class DynamicCache(Cache):\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        self.seen_tokens = 0  # Used in `generate`: how many tokens the cache has seen\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        if layer_idx < len(self):\n",
    "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "        \n",
    "        raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for layer_idx in range(len(self)):\n",
    "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_cache)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if layer_idx == 0:\n",
    "            self.seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        else:\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "    \n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            return 0\n",
    "        \n",
    "        return self.key_cache[layer_idx].shape[-2]\n",
    "    \n",
    "    def get_max_length(self) -> Optional[int]:\n",
    "        return None\n",
    "    \n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n",
    "        for layer_idx in range(len(self.key_cache)):\n",
    "            device = self.key_cache[layer_idx].device\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "\n",
    "            device = self.value_cache[layer_idx].device\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "\n",
    "    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        legacy_cache = ()\n",
    "        for layer_idx in range(len(self)):\n",
    "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
    "\n",
    "        return legacy_cache\n",
    "    \n",
    "    @classmethod\n",
    "    def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n",
    "        cache = cls()\n",
    "        if past_key_values is not None:\n",
    "            for layer_idx in range(len(past_key_values)):\n",
    "                key_states, value_states = past_key_values[layer_idx]\n",
    "                cache.update(\n",
    "                    key_states=key_states,\n",
    "                    value_states=value_states,\n",
    "                    layer_idx=layer_idx,\n",
    "                )\n",
    "        return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralRMSNorm\n",
    "$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}} \\newline$\n",
    "$\\widehat{x} = g\\frac{x_i}{\\sigma}+b$\n",
    "\n",
    "But mistral does not use `bias` in RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRMSNorm(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps=1e-6) -> None:\n",
    "        \"\"\"\n",
    "        The RMSNorm is implemented according `modeling_mistral.py`.\n",
    "        It is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        original_input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "        return self.weight * hidden_states.to(original_input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralRotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size: int,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        base: int = 10000,\n",
    "        device: Optional[Union[torch.device, str]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.theta = 1 / (base ** (torch.arange(0, head_size, 2).float() / head_size))\n",
    "        self.theta = torch.cat([self.theta, self.theta], dim=-1).to(device)\n",
    "        self.position_ids = torch.arange(0, max_position_embeddings).to(device)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, position_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        position_maxtrix = torch.outer(self.position_ids, self.theta)\n",
    "        cos = torch.cos(position_maxtrix)\n",
    "        sin = torch.sin(position_maxtrix)\n",
    "\n",
    "        x1 = hidden_states[..., :hidden_states.shape[-1] // 2]\n",
    "        x2 = hidden_states[..., hidden_states.shape[-1] // 2 :]\n",
    "        _x = torch.cat([-x2, x1], dim=-1)\n",
    "        out = hidden_states * cos[position_ids] + _x * sin[position_ids]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralAttention(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Init\n",
    "        self.num_q_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.num_kv_groups = self.num_q_heads // self.num_kv_heads\n",
    "        self.head_size = config.hidden_size // self.num_q_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        # QKVO Layer\n",
    "        self.q_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.k_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.num_kv_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.v_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.num_kv_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.o_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # RoPE\n",
    "        self.rotary_emb = MistralRotaryEmbedding(\n",
    "            head_size=self.head_size,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            base=config.rope_theta,\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        # Init\n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "\n",
    "        # QKV\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_q_heads, self.head_size).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # KV Cache\n",
    "        kv_seq_len = key_states.size(2)\n",
    "        if past_key_value is not None and self.layer_idx is not None:\n",
    "            kv_seq_len += past_key_value.get_usable_length(\n",
    "                new_seq_length=kv_seq_len,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "\n",
    "        query_states = self.rotary_emb(\n",
    "            hidden_states=query_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        key_states = self.rotary_emb(\n",
    "            hidden_states=key_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            key_states, value_states = past_key_value.update(\n",
    "                key_states=key_states,\n",
    "                value_states=value_states,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "        \n",
    "        # Repeat kv heads\n",
    "        key_states, value_states = repeat_kv(\n",
    "            key_states=key_states,\n",
    "            value_states=value_states,\n",
    "            num_kv_groups=self.num_kv_groups,\n",
    "        )\n",
    "\n",
    "        # Attention weights (Q * K^T)\n",
    "        attention_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
    "\n",
    "        # Attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_weights = attention_weights + attention_mask\n",
    "\n",
    "        # Upcast attention to fp32\n",
    "        attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attention_weights = torch.nn.functional.dropout(attention_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        # Attention output (A = Q * K^T, A * V)\n",
    "        attention_output = torch.matmul(attention_weights, value_states).reshape(batch_size, seq_len, self.hidden_size)\n",
    "        attention_output = self.o_proj(attention_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attention_weights = None\n",
    "\n",
    "        return attention_output, attention_weights, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralSdpaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralSdpaAttention(MistralAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        if output_attentions:\n",
    "            return super().forward(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "        \n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "\n",
    "        # QKV\n",
    "        query_states = self.q_proj(hidden_size)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # KV Cache\n",
    "        kv_seq_len = key_states.size(2)\n",
    "        if past_key_value is not None and self.layer_idx is not None:\n",
    "            kv_seq_len += past_key_value.get_usable_length(\n",
    "                new_seq_length=kv_seq_len,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "\n",
    "        query_states = self.rotary_emb(\n",
    "            hidden_states=query_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        key_states = self.rotary_emb(\n",
    "            hidden_states=key_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            key_states, value_states = past_key_value.update(\n",
    "                key_states=key_states,\n",
    "                value_states=value_states,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "        \n",
    "        # Repeat kv heads\n",
    "        key_states, value_states = repeat_kv(\n",
    "            key_states=key_states,\n",
    "            value_states=value_states,\n",
    "            num_kv_groups=self.num_kv_groups,\n",
    "        )\n",
    "\n",
    "        # Contiguous\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "        # SDPA\n",
    "        attention_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query=query_states,\n",
    "            key=key_states,\n",
    "            value=value_states,\n",
    "            attn_mask=attention_mask,\n",
    "            dropout_p=self.attention_dropout,\n",
    "            is_causal=self.is_causal and attention_mask is None and seq_len > 1,\n",
    "        )\n",
    "\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, seq_len, hidden_size)\n",
    "        attention_output = self.o_proj(attention_output)\n",
    "\n",
    "        return attention_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralMLP(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.gate_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.intermediate_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.up_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.intermediate_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.down_proj = torch.nn.Linear(\n",
    "            in_features=config.intermediate_size,\n",
    "            out_features=config.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.act_fn = torch.nn.functional.gelu\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        up_output = self.up_proj(x)\n",
    "        gate_output = self.gate_proj(x)\n",
    "        intermediate_output = self.act_fn(gate_output) + up_output\n",
    "        down_output = self.down_proj(intermediate_output)\n",
    "        return down_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = MistralSdpaAttention(config=config)\n",
    "        self.mlp = MistralMLP(config=config)\n",
    "        self.input_layernorm = MistralRMSNorm(config.hidden_size)\n",
    "        self.post_attention_layernorm = MistralRMSNorm(config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attention_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        # Redisual connection\n",
    "        hidden_states = hidden_states + residual\n",
    "\n",
    "        # Fully connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attention_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralModel(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = torch.nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "            padding_idx=config.pad_token_id,\n",
    "        )\n",
    "        self.layers = torch.nn.ModuleList([MistralDecoderLayer(config=config)] for _ in range(config.num_hidden_layers))\n",
    "        self.norm = MistralRMSNorm(config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        input_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMistralModelForCausalLM(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.model = MistralModel(config=config)\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            input_features=config.hidden_size,\n",
    "            out_features=config.vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "    ) -> torch.Tensor:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
