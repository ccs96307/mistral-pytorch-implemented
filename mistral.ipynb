{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, OrderedDict, Optional, List, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "\n",
    "import regex as re\n",
    "import torch\n",
    "from transformers import AutoConfig, MistralForCausalLM, LlamaTokenizer\n",
    "from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformers.utils.hub import cached_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"echarlaix/tiny-random-mistral\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "model = MistralForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"echarlaix/tiny-random-mistral\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 32,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 37,\n",
       "  \"is_decoder\": true,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.39.1\",\n",
       "  \"type_vocab_size\": 16,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        _name_or_path: str = \"echarlaix/tiny-random-mistral\",\n",
    "        _attn_implementation: str = \"sdpa\",\n",
    "        architectures: List[str] = [\n",
    "            \"MistralForCausalLM\"\n",
    "        ],\n",
    "        attention_dropout: float = 0.0,\n",
    "        attention_probs_dropout_prob: float = 0.1,\n",
    "        bos_token_id: int = 1,\n",
    "        eos_token_id: int = 2,\n",
    "        hidden_act: str = \"gelu\",\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        hidden_size: int = 32,\n",
    "        initializer_range: float = 0.02,\n",
    "        intermediate_size: int = 37,\n",
    "        is_decoder: bool = True,\n",
    "        max_position_embeddings: int = 512,\n",
    "        model_type: str = \"mistral\",\n",
    "        num_attention_heads: int = 4,\n",
    "        num_hidden_layers: int = 2,\n",
    "        num_key_value_heads: int = 2,\n",
    "        pad_token_id: int = 0,\n",
    "        rms_norm_eps: float = 1e-06,\n",
    "        rope_theta: float = 10000.0,\n",
    "        sliding_window: int = 4096,\n",
    "        tie_word_embeddings: bool = False,\n",
    "        torch_dtype: str = \"float32\",\n",
    "        transformers_version: str = \"4.39.1\",\n",
    "        type_vocab_size: int = 16,\n",
    "        use_cache: bool = True,\n",
    "        vocab_size: int = 32000,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "    ) -> None:\n",
    "        self._name_or_path = _name_or_path\n",
    "        self._attn_implementation = _attn_implementation\n",
    "        self.architectures = architectures\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.hidden_act = hidden_act\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.hidden_size = hidden_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.is_decoder = is_decoder\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.model_type = model_type\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.sliding_window = sliding_window\n",
    "        self.tie_word_embeddings = tie_word_embeddings\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.transformers_version = transformers_version\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.use_cache = use_cache\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_pretrained_model_or_path(pretrained_model_name_or_path: str) -> \"MistralConfig\":\n",
    "        resolved_archive_file = cached_file(\n",
    "            path_or_repo_id=pretrained_model_name_or_path,\n",
    "            filename=CONFIG_NAME,\n",
    "            _raise_exceptions_for_missing_entries=False,\n",
    "        )\n",
    "        \n",
    "        config_content = json.load(open(resolved_archive_file))\n",
    "        return MistralConfig(**config_content)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MistralConfig.from_pretrained_model_or_path(pretrained_model_name_or_path=pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModelOutputWithPast:\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithPast:\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(\n",
    "    key_states: torch.Tensor,\n",
    "    value_states: torch.Tensor,\n",
    "    num_kv_groups: int,\n",
    ") -> torch.Tensor:\n",
    "    batch_size, num_heads, seq_len, head_size = key_states.size()\n",
    "    key_states = key_states[:, :, None, :, :].expand(\n",
    "        batch_size,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        seq_len,\n",
    "        head_size,\n",
    "    )\n",
    "\n",
    "    value_states = value_states[:, :, None, :, :].expand(\n",
    "        batch_size,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        seq_len,\n",
    "        head_size,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        key_states.reshape(batch_size, num_heads * num_kv_groups, seq_len, head_size),\n",
    "        value_states.reshape(batch_size, num_heads * num_kv_groups, seq_len, head_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    \"\"\"Base, abstract class for all caches.\"\"\"\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return the updated key and value states.\"\"\"\n",
    "        raise NotImplementedError(\"Make sure to implement `update` method in a subclass.\")\n",
    "    \n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        raise NotImplementedError(\"Make sure to implement `get_seq_length` in subclass.\")\n",
    "    \n",
    "    def get_max_length(self) -> int:\n",
    "        raise NotImplementedError(\"Make sure to implement `get_max_length` in subclass.\")\n",
    "    \n",
    "    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n",
    "        max_length = self.get_max_length()\n",
    "        previous_seq_length = self.get_seq_length(layer_idx=layer_idx)\n",
    "\n",
    "        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n",
    "            return max_length - new_seq_length\n",
    "        \n",
    "        return previous_seq_length\n",
    "    \n",
    "\n",
    "class DynamicCache(Cache):\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        self.seen_tokens = 0  # Used in `generate`: how many tokens the cache has seen\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        if layer_idx < len(self):\n",
    "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "        \n",
    "        raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for layer_idx in range(len(self)):\n",
    "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_cache)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if layer_idx == 0:\n",
    "            self.seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        else:\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "    \n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            return 0\n",
    "        \n",
    "        return self.key_cache[layer_idx].shape[-2]\n",
    "    \n",
    "    def get_max_length(self) -> Optional[int]:\n",
    "        return None\n",
    "    \n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n",
    "        for layer_idx in range(len(self.key_cache)):\n",
    "            device = self.key_cache[layer_idx].device\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "\n",
    "            device = self.value_cache[layer_idx].device\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "\n",
    "    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        legacy_cache = ()\n",
    "        for layer_idx in range(len(self)):\n",
    "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
    "\n",
    "        return legacy_cache\n",
    "    \n",
    "    @classmethod\n",
    "    def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n",
    "        cache = cls()\n",
    "        if past_key_values is not None:\n",
    "            for layer_idx in range(len(past_key_values)):\n",
    "                key_states, value_states = past_key_values[layer_idx]\n",
    "                cache.update(\n",
    "                    key_states=key_states,\n",
    "                    value_states=value_states,\n",
    "                    layer_idx=layer_idx,\n",
    "                )\n",
    "        return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralRMSNorm\n",
    "$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}} \\newline$\n",
    "$\\widehat{x} = g\\frac{x_i}{\\sigma}+b$\n",
    "\n",
    "But mistral does not use `bias` in RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRMSNorm(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps=1e-6) -> None:\n",
    "        \"\"\"\n",
    "        The RMSNorm is implemented according `modeling_mistral.py`.\n",
    "        It is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        original_input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "        return self.weight * hidden_states.to(original_input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralRotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size: int,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        base: int = 10000,\n",
    "        device: Optional[Union[torch.device, str]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.theta = 1 / (base ** (torch.arange(0, head_size, 2).float() / head_size))\n",
    "        self.theta = torch.cat([self.theta, self.theta], dim=-1).to(device)\n",
    "        self.position_ids = torch.arange(0, max_position_embeddings).to(device)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, position_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        position_maxtrix = torch.outer(self.position_ids, self.theta)\n",
    "        cos = torch.cos(position_maxtrix)\n",
    "        sin = torch.sin(position_maxtrix)\n",
    "\n",
    "        x1 = hidden_states[..., :hidden_states.shape[-1] // 2]\n",
    "        x2 = hidden_states[..., hidden_states.shape[-1] // 2 :]\n",
    "        _x = torch.cat([-x2, x1], dim=-1)\n",
    "        out = hidden_states * cos[position_ids] + _x * sin[position_ids]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralAttention(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Init\n",
    "        self.num_q_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.num_kv_groups = self.num_q_heads // self.num_kv_heads\n",
    "        self.head_size = config.hidden_size // self.num_q_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        # QKVO Layer\n",
    "        self.q_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.k_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.num_kv_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.v_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.num_kv_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.o_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # RoPE\n",
    "        self.rotary_emb = MistralRotaryEmbedding(\n",
    "            head_size=self.head_size,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            base=config.rope_theta,\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        # Init\n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "\n",
    "        # QKV\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_q_heads, self.head_size).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # KV Cache\n",
    "        kv_seq_len = key_states.size(2)\n",
    "        if past_key_value is not None and self.layer_idx is not None:\n",
    "            kv_seq_len += past_key_value.get_usable_length(\n",
    "                new_seq_length=kv_seq_len,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "\n",
    "        query_states = self.rotary_emb(\n",
    "            hidden_states=query_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        key_states = self.rotary_emb(\n",
    "            hidden_states=key_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            key_states, value_states = past_key_value.update(\n",
    "                key_states=key_states,\n",
    "                value_states=value_states,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "        \n",
    "        # Repeat kv heads\n",
    "        key_states, value_states = repeat_kv(\n",
    "            key_states=key_states,\n",
    "            value_states=value_states,\n",
    "            num_kv_groups=self.num_kv_groups,\n",
    "        )\n",
    "\n",
    "        # Attention weights (Q * K^T)\n",
    "        attention_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
    "\n",
    "        # Attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_weights = attention_weights + attention_mask\n",
    "\n",
    "        # Upcast attention to fp32\n",
    "        attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attention_weights = torch.nn.functional.dropout(attention_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        # Attention output (A = Q * K^T, A * V)\n",
    "        attention_output = torch.matmul(attention_weights, value_states).reshape(batch_size, seq_len, self.hidden_size)\n",
    "        attention_output = self.o_proj(attention_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attention_weights = None\n",
    "\n",
    "        return attention_output, attention_weights, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralSdpaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralSdpaAttention(MistralAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        if output_attentions:\n",
    "            return super().forward(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "        \n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "\n",
    "        # QKV\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_q_heads, self.head_size).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # KV Cache\n",
    "        kv_seq_len = key_states.size(2)\n",
    "        if past_key_value is not None and self.layer_idx is not None:\n",
    "            kv_seq_len += past_key_value.get_usable_length(\n",
    "                new_seq_length=kv_seq_len,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "\n",
    "        query_states = self.rotary_emb(\n",
    "            hidden_states=query_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        key_states = self.rotary_emb(\n",
    "            hidden_states=key_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            key_states, value_states = past_key_value.update(\n",
    "                key_states=key_states,\n",
    "                value_states=value_states,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "        \n",
    "        # Repeat kv heads\n",
    "        key_states, value_states = repeat_kv(\n",
    "            key_states=key_states,\n",
    "            value_states=value_states,\n",
    "            num_kv_groups=self.num_kv_groups,\n",
    "        )\n",
    "\n",
    "        # Contiguous\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "        # SDPA\n",
    "        attention_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query=query_states,\n",
    "            key=key_states,\n",
    "            value=value_states,\n",
    "            attn_mask=attention_mask,\n",
    "            dropout_p=self.attention_dropout,\n",
    "            is_causal=self.is_causal and attention_mask is None and seq_len > 1,\n",
    "        )\n",
    "\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, seq_len, hidden_size)\n",
    "        attention_output = self.o_proj(attention_output)\n",
    "\n",
    "        return attention_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralMLP(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.gate_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.intermediate_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.up_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.intermediate_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.down_proj = torch.nn.Linear(\n",
    "            in_features=config.intermediate_size,\n",
    "            out_features=config.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.act_fn = torch.nn.functional.gelu\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        up_output = self.up_proj(x)\n",
    "        gate_output = self.gate_proj(x)\n",
    "        intermediate_output = self.act_fn(gate_output) + up_output\n",
    "        down_output = self.down_proj(intermediate_output)\n",
    "        return down_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: int) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = MistralSdpaAttention(config=config, layer_idx=layer_idx)\n",
    "        self.mlp = MistralMLP(config=config)\n",
    "        self.input_layernorm = MistralRMSNorm(config.hidden_size)\n",
    "        self.post_attention_layernorm = MistralRMSNorm(config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attention_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        # Redisual connection\n",
    "        hidden_states = hidden_states + residual\n",
    "\n",
    "        # Fully connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attention_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _my_prepare_4d_causal_attention_mask_for_sdpa(\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    input_shape: Union[torch.Size, Tuple, List],\n",
    "    inputs_embeds: torch.Tensor,\n",
    "    past_key_values_length: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the correct `attn_mask` argument to be used by `torch.nn.functional.scaled_dot_product_attention`.\n",
    "    \"\"\"\n",
    "    batch_size, query_length = input_shape\n",
    "    key_value_length = query_length + past_key_values_length\n",
    "\n",
    "    if attention_mask is None:\n",
    "        # Creating a causal mask for all positions\n",
    "        mask = torch.full((query_length, key_value_length), torch.finfo(inputs_embeds.dtype).min, device=inputs_embeds.device)\n",
    "        mask_cond = torch.arange(key_value_length, device=inputs_embeds.device)\n",
    "        mask[:, :query_length] = (mask_cond[None, :] < (mask_cond[:query_length] + 1)[:, None]).float()\n",
    "        expanded_4d_mask = mask[None, None, :, :].expand(batch_size, 1, query_length, key_value_length)\n",
    "    elif len(attention_mask.shape) == 4:\n",
    "        # If a 4D attention mask is already provided, use it directly\n",
    "        expanded_4d_mask = attention_mask\n",
    "    else:\n",
    "        # Expanding a 2D mask to 4D\n",
    "        expanded_mask = attention_mask[:, None, None, :]\n",
    "        expanded_4d_mask = expanded_mask.expand(batch_size, 1, query_length, key_value_length).to(dtype=inputs_embeds.dtype)\n",
    "        # Apply causal mask to the expanded mask\n",
    "        causal_mask = torch.triu(torch.ones((query_length, key_value_length), device=inputs_embeds.device, dtype=torch.bool), diagonal=1)\n",
    "        padding_mask = attention_mask == 0\n",
    "        padding_mask = padding_mask.view(batch_size, 1, 1, key_value_length)\n",
    "        expanded_4d_mask = expanded_4d_mask.masked_fill(~padding_mask, 0.)\n",
    "        expanded_4d_mask = expanded_4d_mask.masked_fill(padding_mask, torch.finfo(inputs_embeds.dtype).min)\n",
    "        expanded_4d_mask = expanded_4d_mask.masked_fill(causal_mask, torch.finfo(inputs_embeds.dtype).min)\n",
    "\n",
    "    return expanded_4d_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralModel(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "\n",
    "        self.embed_tokens = torch.nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "            padding_idx=config.pad_token_id,\n",
    "        )\n",
    "        self.layers = torch.nn.ModuleList([MistralDecoderLayer(config=config, layer_idx=layer_idx) for layer_idx in range(config.num_hidden_layers)])\n",
    "        self.norm = MistralRMSNorm(config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        # Config\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        # Init\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        past_key_values_length = 0\n",
    "\n",
    "        # If use cache\n",
    "        if use_cache:\n",
    "            use_legacy_cache = not isinstance(past_key_values, Cache)\n",
    "            if use_legacy_cache:\n",
    "                past_key_values = DynamicCache.from_legacy_cache(past_key_values=past_key_values)\n",
    "\n",
    "            past_key_values_length = past_key_values.get_usable_length(seq_length)\n",
    "\n",
    "        # Position ids\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(\n",
    "                start=past_key_values_length,\n",
    "                end=past_key_values_length + seq_length,\n",
    "                dtype=torch.long,\n",
    "                device=input_ids.device,\n",
    "            )\n",
    "        else:\n",
    "            position_ids = position_ids.view(-1, seq_length).long()\n",
    "\n",
    "        # Input embedding\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        # Attention mask (output_attentions is not supported when using SDPA)\n",
    "        if self._attn_implementation == \"sdpa\" and not output_attentions:\n",
    "            attention_mask = _my_prepare_4d_causal_attention_mask_for_sdpa(\n",
    "                attention_mask=attention_mask,\n",
    "                input_shape=(batch_size, seq_length),\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"_my_prepare_4d_causal_attention_mask() if not implemented for now.\",\n",
    "            )\n",
    "        \n",
    "        # Feed-Forward\n",
    "        hidden_states = inputs_embeds\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "\n",
    "            # No matter how many data returned, the first one is the `hidden_states`\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions += (layer_outputs[1],)\n",
    "        \n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # Add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n",
    "\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMistralForCausalLM(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = MistralModel(config=config)\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self) -> None:\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        # Settings\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n",
    "\n",
    "        # decoder outputs\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,   \n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.lm_head(hidden_states).float()\n",
    "\n",
    "        # Loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            # Shift\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            # Flatten\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "\n",
    "            # Make sure they are on the same device\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = criterion(shift_logits, shift_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=past_key_values,\n",
    "            hidden_states=hidden_states,\n",
    "            attentions=attention_mask,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name_or_path: str) -> \"MyMistralForCausalLM\":\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "        \n",
    "        Args:\n",
    "            pretrained_model_name_or_path: One of\n",
    "                * \"echarlaix/tiny-random-mistral\"\n",
    "                ...\n",
    "\n",
    "        Returns:\n",
    "            model: MyMistralModelForCausalLM model with weights loaded\n",
    "        \"\"\"\n",
    "\n",
    "        def load_state_dict_hf(path_or_repo_id: str) -> OrderedDict:\n",
    "            resolved_archive_file = cached_file(\n",
    "                path_or_repo_id=path_or_repo_id,\n",
    "                filename=WEIGHTS_NAME,\n",
    "            )\n",
    "            return torch.load(resolved_archive_file, weights_only=True)\n",
    "\n",
    "        # Load config\n",
    "        config = MistralConfig.from_pretrained_model_or_path(pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "\n",
    "        # Load weights\n",
    "        state_dict = load_state_dict_hf(pretrained_model_name_or_path)\n",
    "\n",
    "        # Load model\n",
    "        model = MyMistralForCausalLM(config=config)\n",
    "        model.load_state_dict(state_dict=state_dict, strict=True)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = MyMistralForCausalLM.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Today is a nice day.\",\n",
    "    \"I want to go to play, do you want to join us?\",\n",
    "    \"???\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 0.0572,  0.3510,  0.1318,  ..., -0.2449, -0.0390, -0.0728],\n",
       "         [ 0.0973,  0.0157,  0.1455,  ..., -0.0860, -0.1055, -0.1053],\n",
       "         [ 0.1604,  0.1928,  0.0469,  ..., -0.0620,  0.0588, -0.0535],\n",
       "         ...,\n",
       "         [ 0.3447, -0.1340, -0.1853,  ...,  0.0474, -0.0168, -0.2040],\n",
       "         [ 0.3447, -0.1342, -0.1855,  ...,  0.0477, -0.0167, -0.2039],\n",
       "         [ 0.3446, -0.1344, -0.1857,  ...,  0.0479, -0.0166, -0.2038]],\n",
       "\n",
       "        [[ 0.0572,  0.3510,  0.1318,  ..., -0.2449, -0.0390, -0.0728],\n",
       "         [-0.0568, -0.2399,  0.0718,  ...,  0.1150, -0.1147, -0.0202],\n",
       "         [-0.0531,  0.0850,  0.0348,  ..., -0.1261,  0.1661,  0.1647],\n",
       "         ...,\n",
       "         [ 0.1463, -0.1691, -0.1373,  ...,  0.1508, -0.0556,  0.0063],\n",
       "         [-0.0097, -0.0065,  0.2922,  ..., -0.0257, -0.1324,  0.0456],\n",
       "         [-0.0217, -0.0284,  0.1220,  ...,  0.0391, -0.1025, -0.2051]],\n",
       "\n",
       "        [[ 0.0572,  0.3510,  0.1318,  ..., -0.2449, -0.0390, -0.0728],\n",
       "         [ 0.0700, -0.1413,  0.0275,  ...,  0.1097, -0.1339, -0.1933],\n",
       "         [-0.0758, -0.0792, -0.0158,  ...,  0.0405,  0.0100,  0.0218],\n",
       "         ...,\n",
       "         [ 0.3791, -0.1516, -0.1903,  ...,  0.1046, -0.0257, -0.2503],\n",
       "         [ 0.3792, -0.1515, -0.1903,  ...,  0.1048, -0.0256, -0.2504],\n",
       "         [ 0.3793, -0.1515, -0.1902,  ...,  0.1049, -0.0256, -0.2501]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=None, hidden_states=tensor([[[-2.1384e+00, -1.5581e+00, -2.0885e+00,  ..., -7.0048e-01,\n",
       "          -4.1863e-01, -6.0643e-01],\n",
       "         [-8.8862e-01, -5.3158e-01,  5.6839e-01,  ..., -8.5103e-01,\n",
       "           5.0393e-01, -6.7799e-01],\n",
       "         [ 8.2012e-04,  4.5010e-01, -2.2387e+00,  ..., -7.3072e-01,\n",
       "          -6.2089e-02,  1.0878e+00],\n",
       "         ...,\n",
       "         [-4.2270e-01, -1.0988e+00, -2.1940e-01,  ...,  2.2449e+00,\n",
       "           4.5369e-01,  4.7011e-01],\n",
       "         [-4.2167e-01, -1.0994e+00, -2.1753e-01,  ...,  2.2458e+00,\n",
       "           4.5291e-01,  4.6911e-01],\n",
       "         [-4.2241e-01, -1.0989e+00, -2.1671e-01,  ...,  2.2468e+00,\n",
       "           4.5190e-01,  4.6786e-01]],\n",
       "\n",
       "        [[-2.1384e+00, -1.5581e+00, -2.0885e+00,  ..., -7.0048e-01,\n",
       "          -4.1863e-01, -6.0643e-01],\n",
       "         [ 8.8991e-01,  7.8279e-01,  4.2527e-01,  ..., -5.5639e-01,\n",
       "          -2.8949e-01,  6.2841e-01],\n",
       "         [-3.4219e-01, -4.2103e-01, -3.2015e-01,  ...,  9.2583e-02,\n",
       "           2.0202e-01,  6.1853e-02],\n",
       "         ...,\n",
       "         [-1.1921e-01, -9.1035e-01,  1.8013e+00,  ...,  2.2829e+00,\n",
       "           4.6675e-01, -9.9496e-01],\n",
       "         [ 1.1196e+00,  1.0176e+00, -7.4965e-01,  ..., -1.4327e+00,\n",
       "           1.7161e+00, -1.8957e+00],\n",
       "         [-8.6825e-01, -5.8460e-01, -1.3342e+00,  ..., -1.9420e+00,\n",
       "           7.7007e-01,  2.8057e-01]],\n",
       "\n",
       "        [[-2.1384e+00, -1.5581e+00, -2.0885e+00,  ..., -7.0048e-01,\n",
       "          -4.1863e-01, -6.0643e-01],\n",
       "         [ 8.7207e-02, -6.3834e-01,  4.0735e-01,  ..., -1.7042e+00,\n",
       "          -3.0182e-01,  3.5432e-01],\n",
       "         [-1.2820e-01, -1.2469e-02,  1.8711e+00,  ..., -1.2242e-01,\n",
       "          -4.2282e-01,  2.7167e-01],\n",
       "         ...,\n",
       "         [-6.1425e-01, -1.2122e+00, -4.2005e-01,  ...,  2.0658e+00,\n",
       "           7.4807e-02,  3.7553e-01],\n",
       "         [-6.1212e-01, -1.2124e+00, -4.2687e-01,  ...,  2.0653e+00,\n",
       "           7.4728e-02,  3.7560e-01],\n",
       "         [-6.0906e-01, -1.2111e+00, -4.3080e-01,  ...,  2.0638e+00,\n",
       "           7.5867e-02,  3.7550e-01]]], grad_fn=<MulBackward0>), attentions=tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 0.1035,  0.1514, -0.0029,  ..., -0.1748,  0.0884, -0.0410],\n",
       "         [-0.0264,  0.1084,  0.2236,  ..., -0.1660, -0.0938, -0.0349],\n",
       "         [-0.1338, -0.0393,  0.0269,  ...,  0.0515, -0.0090, -0.0048],\n",
       "         ...,\n",
       "         [ 0.0952,  0.0056,  0.0962,  ..., -0.0537, -0.0767, -0.0312],\n",
       "         [ 0.0957,  0.0058,  0.0962,  ..., -0.0537, -0.0762, -0.0320],\n",
       "         [ 0.0952,  0.0051,  0.0962,  ..., -0.0537, -0.0757, -0.0320]],\n",
       "\n",
       "        [[ 0.1035,  0.1514, -0.0029,  ..., -0.1748,  0.0884, -0.0410],\n",
       "         [-0.0010, -0.1348,  0.0942,  ..., -0.0315, -0.0618,  0.0996],\n",
       "         [-0.1904,  0.0284,  0.0840,  ..., -0.0344, -0.0830,  0.0430],\n",
       "         ...,\n",
       "         [-0.1064, -0.1475,  0.0023,  ..., -0.1108,  0.0693, -0.0087],\n",
       "         [-0.1084, -0.2188, -0.1719,  ...,  0.0064,  0.0845,  0.0054],\n",
       "         [-0.1079,  0.0247,  0.1157,  ..., -0.0403, -0.0820,  0.0530]],\n",
       "\n",
       "        [[ 0.1035,  0.1514, -0.0029,  ..., -0.1748,  0.0884, -0.0410],\n",
       "         [ 0.0034, -0.0334,  0.0515,  ..., -0.1572, -0.0162, -0.0879],\n",
       "         [-0.0500,  0.0605,  0.0938,  ...,  0.0057, -0.0192,  0.0011],\n",
       "         ...,\n",
       "         [-0.0041, -0.0157,  0.0908,  ..., -0.1465,  0.0396,  0.0664],\n",
       "         [-0.0040, -0.0165,  0.0908,  ..., -0.1465,  0.0396,  0.0664],\n",
       "         [-0.0028, -0.0172,  0.0903,  ..., -0.1455,  0.0383,  0.0654]]],\n",
       "       grad_fn=<ToCopyBackward0>), past_key_values=((tensor([[[[ 6.2988e-02,  3.2471e-02,  9.8145e-02,  1.0303e-01, -1.2695e-01,\n",
       "           -2.0020e-02,  3.2959e-02,  1.8555e-01],\n",
       "          [ 3.1250e-02, -6.7139e-03, -8.2520e-02, -1.9043e-01, -1.5723e-01,\n",
       "            1.3281e-01, -1.8262e-01,  5.9814e-02],\n",
       "          [ 6.4453e-02,  7.6172e-02, -3.2227e-02,  2.5635e-02, -2.7734e-01,\n",
       "            3.1641e-01, -1.1328e-01,  1.3672e-01],\n",
       "          [-1.0889e-01, -7.6172e-02,  2.3682e-02,  1.0400e-01, -9.7656e-04,\n",
       "           -8.8867e-02, -8.0566e-02, -5.0781e-02],\n",
       "          [ 9.8633e-02,  4.9805e-02,  5.1758e-02, -1.3428e-02,  2.2461e-01,\n",
       "            2.4048e-02, -6.9336e-02, -8.2520e-02],\n",
       "          [-3.9551e-02, -2.1094e-01, -4.8340e-02,  1.3794e-02, -9.2773e-02,\n",
       "           -7.2266e-02, -4.4189e-02,  9.2773e-03],\n",
       "          [-9.5215e-02, -1.8433e-02,  1.5527e-01,  9.0332e-02,  3.1982e-02,\n",
       "            3.3691e-02, -1.0400e-01,  3.2617e-01],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 2.3071e-02,  1.3770e-01,  1.2695e-02, -3.1738e-02,  1.2109e-01,\n",
       "            1.2402e-01,  1.6602e-01, -2.5879e-02],\n",
       "          [ 4.8828e-03, -2.5879e-02, -5.7861e-02,  1.9141e-01,  1.6235e-02,\n",
       "           -5.5176e-02, -1.9165e-02,  1.7578e-01],\n",
       "          [ 2.8931e-02, -1.9531e-01,  1.0742e-02, -4.2480e-02, -4.8340e-02,\n",
       "           -7.0312e-02, -8.6914e-02,  1.0352e-01],\n",
       "          [ 1.2695e-01, -1.9922e-01, -3.7842e-02,  1.1182e-01, -4.5898e-02,\n",
       "           -9.7656e-03, -1.5723e-01, -1.2793e-01],\n",
       "          [ 8.3008e-02, -3.8086e-02, -2.3145e-01, -5.1758e-02, -1.1621e-01,\n",
       "           -1.7773e-01, -1.0693e-01, -1.8457e-01],\n",
       "          [-1.2451e-01,  1.0938e-01, -4.6387e-02, -7.8125e-02,  5.2979e-02,\n",
       "            1.7773e-01,  1.4404e-02, -6.1340e-03],\n",
       "          [-1.0059e-01,  7.2754e-02,  5.1880e-03,  5.2979e-02, -2.6367e-01,\n",
       "           -8.3008e-02,  8.2397e-03,  1.8652e-01],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 6.2988e-02,  3.2471e-02,  9.8145e-02,  1.0303e-01, -1.2695e-01,\n",
       "           -2.0020e-02,  3.2959e-02,  1.8555e-01],\n",
       "          [ 5.5908e-02, -1.0498e-01, -5.5664e-02, -2.3926e-02,  1.9336e-01,\n",
       "           -2.5391e-01,  4.9744e-03, -5.4932e-02],\n",
       "          [-5.2979e-02,  8.1055e-02, -1.4062e-01,  2.3193e-02, -1.6992e-01,\n",
       "           -2.1484e-02,  1.1621e-01, -6.3477e-03],\n",
       "          [ 1.8750e-01,  5.3467e-02,  1.3184e-01, -2.0703e-01, -2.4414e-04,\n",
       "            1.4941e-01, -1.2891e-01,  4.8340e-02],\n",
       "          [ 1.0352e-01,  1.6724e-02, -1.5039e-01, -5.1270e-03,  1.9629e-01,\n",
       "            6.8848e-02, -2.4780e-02, -2.5879e-02],\n",
       "          [-7.7637e-02,  2.2705e-02,  1.3477e-01, -2.0703e-01,  1.6992e-01,\n",
       "            1.5723e-01, -1.2598e-01,  4.8096e-02],\n",
       "          [ 5.7129e-02, -4.7363e-02, -8.6670e-03,  2.6758e-01,  1.7285e-01,\n",
       "           -8.9844e-02,  3.9062e-02,  6.2500e-02],\n",
       "          [ 4.1992e-02,  4.3945e-03, -1.3306e-02,  9.0820e-02, -1.0059e-01,\n",
       "           -4.1016e-02, -1.0205e-01, -9.4238e-02],\n",
       "          [-4.0283e-02, -5.7617e-02, -3.7842e-02, -4.0527e-02, -2.5757e-02,\n",
       "            1.0059e-01, -1.1865e-01,  6.7871e-02],\n",
       "          [ 8.0078e-02,  4.9561e-02,  1.3281e-01,  5.0537e-02,  5.7373e-03,\n",
       "            3.1982e-02,  3.8574e-02,  1.2793e-01],\n",
       "          [ 1.7578e-01,  7.1777e-02, -1.4941e-01,  2.3315e-02, -2.7832e-02,\n",
       "            4.2969e-02,  1.0498e-01, -6.1646e-03],\n",
       "          [-2.7222e-02, -7.0312e-02,  1.4160e-01, -2.0801e-01,  1.8555e-01,\n",
       "            1.4258e-01, -1.1768e-01,  4.6875e-02],\n",
       "          [-9.6191e-02, -2.0020e-01, -1.4844e-01, -8.6914e-02,  2.7710e-02,\n",
       "           -3.1738e-02, -2.1582e-01, -5.6641e-02],\n",
       "          [ 1.3379e-01,  3.9062e-02, -7.2266e-02, -7.8125e-02,  1.3672e-01,\n",
       "            1.0986e-02,  8.6426e-02, -6.6406e-02],\n",
       "          [ 2.6611e-02,  1.7480e-01,  1.9531e-02, -9.5215e-02,  1.1084e-01,\n",
       "           -1.0645e-01, -4.5898e-02,  1.0547e-01]],\n",
       "\n",
       "         [[ 2.3071e-02,  1.3770e-01,  1.2695e-02, -3.1738e-02,  1.2109e-01,\n",
       "            1.2402e-01,  1.6602e-01, -2.5879e-02],\n",
       "          [-9.3994e-03, -7.3730e-02,  1.6992e-01, -7.8735e-03, -1.9531e-02,\n",
       "            6.0547e-02, -3.8818e-02,  1.0449e-01],\n",
       "          [-2.2217e-02,  3.6133e-02,  5.2979e-02, -1.2207e-01,  8.5449e-02,\n",
       "            1.2573e-02, -1.0156e-01, -1.1621e-01],\n",
       "          [-4.3945e-03,  8.5449e-03, -1.2451e-01,  1.2207e-01,  1.3184e-01,\n",
       "           -1.4746e-01, -1.2061e-01,  1.6992e-01],\n",
       "          [-5.8594e-03, -1.0400e-01,  1.3428e-03, -5.2490e-02, -8.6914e-02,\n",
       "           -1.8164e-01, -1.1035e-01, -4.7913e-03],\n",
       "          [-1.1865e-01,  3.7598e-02, -1.2207e-01,  1.2158e-01, -5.9326e-02,\n",
       "           -1.4160e-01, -1.2305e-01,  1.7090e-01],\n",
       "          [-3.4668e-02,  1.2012e-01, -1.2402e-01, -7.7148e-02,  2.3438e-02,\n",
       "            1.0352e-01, -1.1816e-01, -2.0703e-01],\n",
       "          [-6.1523e-02, -1.0889e-01, -1.7871e-01, -2.7222e-02,  2.1094e-01,\n",
       "           -1.4258e-01, -1.4844e-01, -9.6680e-02],\n",
       "          [ 4.0039e-02, -1.1523e-01, -1.4746e-01,  6.4453e-02,  3.0273e-02,\n",
       "           -4.3701e-02, -3.2959e-02,  4.1260e-02],\n",
       "          [ 2.5024e-02,  2.8516e-01, -5.9082e-02,  4.3213e-02,  1.2988e-01,\n",
       "            8.6426e-02, -5.4199e-02,  6.2256e-02],\n",
       "          [-8.1055e-02,  1.6113e-02,  6.1035e-02, -1.2109e-01, -3.4180e-02,\n",
       "            3.4424e-02, -9.7168e-02, -1.1719e-01],\n",
       "          [-1.3086e-01,  1.1133e-01, -1.1426e-01,  1.2061e-01, -2.3804e-02,\n",
       "           -9.5703e-02, -1.2988e-01,  1.7090e-01],\n",
       "          [ 2.1484e-02, -4.0527e-02,  3.3203e-02,  5.4321e-03,  1.4648e-02,\n",
       "           -1.9434e-01,  3.0029e-02,  7.8613e-02],\n",
       "          [-6.8359e-02, -1.6992e-01,  4.7607e-02, -4.0527e-02,  1.2695e-02,\n",
       "            1.3184e-01,  7.8125e-02, -1.6211e-01],\n",
       "          [ 5.4688e-02,  2.4512e-01,  1.1865e-01,  1.8750e-01, -8.7891e-02,\n",
       "           -1.4160e-01, -1.9043e-02, -2.2461e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.2988e-02,  3.2471e-02,  9.8145e-02,  1.0303e-01, -1.2695e-01,\n",
       "           -2.0020e-02,  3.2959e-02,  1.8555e-01],\n",
       "          [ 1.3770e-01, -1.6211e-01,  5.3711e-02, -2.5879e-02, -7.0312e-02,\n",
       "            1.2695e-02, -1.1414e-02, -3.1250e-02],\n",
       "          [ 5.9326e-02,  7.7209e-03,  1.8164e-01,  1.5723e-01,  1.5430e-01,\n",
       "            2.5269e-02, -1.2988e-01, -2.0508e-02],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 2.3071e-02,  1.3770e-01,  1.2695e-02, -3.1738e-02,  1.2109e-01,\n",
       "            1.2402e-01,  1.6602e-01, -2.5879e-02],\n",
       "          [-2.9297e-02, -2.1191e-01, -1.0376e-02, -2.1094e-01, -2.1973e-02,\n",
       "            7.3242e-02,  2.1240e-02,  3.5547e-01],\n",
       "          [ 1.2598e-01, -9.5215e-02, -4.7302e-04,  1.0693e-01,  2.4414e-03,\n",
       "            2.9175e-02, -1.2402e-01, -1.6602e-02],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]]]], dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-0.0898, -0.1079, -0.1924,  0.0498, -0.1494, -0.0703,  0.0155,\n",
       "            0.1523],\n",
       "          [ 0.0830, -0.0192, -0.0315, -0.1494, -0.0073, -0.0991,  0.0535,\n",
       "           -0.1738],\n",
       "          [-0.0674, -0.0018,  0.0796, -0.0141,  0.1807, -0.1123,  0.1934,\n",
       "            0.0649],\n",
       "          [-0.0115, -0.0337, -0.0337,  0.0684, -0.1836,  0.0020,  0.0664,\n",
       "           -0.0850],\n",
       "          [ 0.0498, -0.1660,  0.0369, -0.0334, -0.0613,  0.0781, -0.0121,\n",
       "           -0.0354],\n",
       "          [ 0.1299, -0.0859, -0.1162, -0.0728,  0.0688, -0.0432, -0.0430,\n",
       "           -0.1104],\n",
       "          [ 0.1377,  0.1396,  0.0422, -0.0442,  0.2070, -0.1650,  0.0242,\n",
       "           -0.0457],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]],\n",
       "\n",
       "         [[ 0.1562, -0.1128,  0.2539, -0.1406, -0.0645, -0.0693, -0.0432,\n",
       "           -0.1270],\n",
       "          [ 0.0806, -0.1738, -0.0136,  0.0520,  0.1133,  0.0121, -0.0479,\n",
       "           -0.0610],\n",
       "          [ 0.1221,  0.0060, -0.0525, -0.0096, -0.0708, -0.0352, -0.0166,\n",
       "            0.1006],\n",
       "          [-0.0977, -0.0562, -0.0525, -0.1338, -0.0247,  0.0398,  0.0508,\n",
       "           -0.0059],\n",
       "          [-0.1582, -0.0486, -0.0386,  0.0272,  0.1494, -0.0417,  0.0197,\n",
       "           -0.1836],\n",
       "          [ 0.0869, -0.2041, -0.0752,  0.0552,  0.0170, -0.1167, -0.0703,\n",
       "           -0.0737],\n",
       "          [ 0.1396,  0.0067, -0.1582,  0.2432,  0.0613, -0.0981, -0.0248,\n",
       "           -0.0062],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0898, -0.1079, -0.1924,  0.0498, -0.1494, -0.0703,  0.0155,\n",
       "            0.1523],\n",
       "          [-0.0054,  0.0933, -0.0596,  0.0162, -0.0142,  0.1279,  0.0547,\n",
       "           -0.1260],\n",
       "          [-0.0630, -0.1484, -0.0659, -0.1040,  0.2754, -0.0058,  0.0903,\n",
       "           -0.1572],\n",
       "          [ 0.0728,  0.0090,  0.1885,  0.0400,  0.1216, -0.0986,  0.1299,\n",
       "           -0.0439],\n",
       "          [ 0.0535, -0.3574,  0.1494,  0.0491,  0.1289,  0.1177, -0.0547,\n",
       "           -0.1104],\n",
       "          [ 0.0728,  0.0090,  0.1885,  0.0400,  0.1216, -0.0986,  0.1299,\n",
       "           -0.0439],\n",
       "          [ 0.0376, -0.0055, -0.1040,  0.1045,  0.1416,  0.0287,  0.0554,\n",
       "            0.0688],\n",
       "          [-0.0474,  0.0110,  0.1206, -0.0337,  0.0850,  0.0520,  0.0737,\n",
       "           -0.0062],\n",
       "          [-0.0112, -0.0835, -0.0649, -0.0547,  0.1543, -0.1396,  0.0786,\n",
       "           -0.0972],\n",
       "          [-0.0117,  0.0757, -0.0287,  0.0889, -0.1162,  0.1064, -0.1699,\n",
       "            0.1543],\n",
       "          [-0.0630, -0.1484, -0.0659, -0.1040,  0.2754, -0.0058,  0.0903,\n",
       "           -0.1572],\n",
       "          [ 0.0728,  0.0090,  0.1885,  0.0400,  0.1216, -0.0986,  0.1299,\n",
       "           -0.0439],\n",
       "          [ 0.0330,  0.0007, -0.0302, -0.1904,  0.1934,  0.0320,  0.1113,\n",
       "           -0.1367],\n",
       "          [-0.1299, -0.0649, -0.1182, -0.1201,  0.0581, -0.1904,  0.0581,\n",
       "           -0.0537],\n",
       "          [ 0.2031,  0.0332,  0.0193, -0.0620, -0.0491, -0.0075, -0.0757,\n",
       "            0.0674]],\n",
       "\n",
       "         [[ 0.1562, -0.1128,  0.2539, -0.1406, -0.0645, -0.0693, -0.0432,\n",
       "           -0.1270],\n",
       "          [ 0.0266,  0.0630, -0.2139, -0.0620, -0.1553,  0.1582, -0.0011,\n",
       "            0.1177],\n",
       "          [-0.1533, -0.0957, -0.1309,  0.1079,  0.0481,  0.0227,  0.0038,\n",
       "           -0.0265],\n",
       "          [ 0.0359, -0.0405, -0.0791,  0.0723, -0.0109, -0.0569,  0.1182,\n",
       "           -0.0693],\n",
       "          [-0.1582, -0.0104, -0.0026, -0.0513,  0.0767, -0.0977,  0.0820,\n",
       "            0.0752],\n",
       "          [ 0.0359, -0.0405, -0.0791,  0.0723, -0.0109, -0.0569,  0.1182,\n",
       "           -0.0693],\n",
       "          [-0.0918, -0.0215, -0.0540,  0.0205, -0.0222,  0.0272, -0.0040,\n",
       "           -0.2012],\n",
       "          [-0.0815,  0.0500,  0.0398,  0.1934,  0.0771, -0.0312,  0.0211,\n",
       "           -0.0273],\n",
       "          [ 0.0449, -0.1689, -0.0312,  0.0082,  0.0562,  0.2969, -0.0349,\n",
       "           -0.0820],\n",
       "          [ 0.0437, -0.0332, -0.0306, -0.0376,  0.0226,  0.0938, -0.1152,\n",
       "           -0.2354],\n",
       "          [-0.1533, -0.0957, -0.1309,  0.1079,  0.0481,  0.0227,  0.0038,\n",
       "           -0.0265],\n",
       "          [ 0.0359, -0.0405, -0.0791,  0.0723, -0.0109, -0.0569,  0.1182,\n",
       "           -0.0693],\n",
       "          [ 0.1279, -0.0332, -0.0640,  0.1748,  0.0815,  0.1206, -0.0038,\n",
       "            0.1016],\n",
       "          [ 0.1045, -0.1494,  0.0620,  0.0649,  0.0339,  0.2246, -0.0669,\n",
       "           -0.0053],\n",
       "          [ 0.0835,  0.0569,  0.1221, -0.0903,  0.0718,  0.0542,  0.1768,\n",
       "            0.1699]]],\n",
       "\n",
       "\n",
       "        [[[-0.0898, -0.1079, -0.1924,  0.0498, -0.1494, -0.0703,  0.0155,\n",
       "            0.1523],\n",
       "          [ 0.0859, -0.0698,  0.0304,  0.0698, -0.2363,  0.0630, -0.0703,\n",
       "            0.0591],\n",
       "          [ 0.1934, -0.1768,  0.1318, -0.0188,  0.0339, -0.1807,  0.1504,\n",
       "           -0.0334],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]],\n",
       "\n",
       "         [[ 0.1562, -0.1128,  0.2539, -0.1406, -0.0645, -0.0693, -0.0432,\n",
       "           -0.1270],\n",
       "          [ 0.1045,  0.1182,  0.0188,  0.1777, -0.0120, -0.1367, -0.1641,\n",
       "            0.0549],\n",
       "          [-0.0693, -0.1357,  0.0859, -0.1797, -0.0284, -0.0298,  0.0898,\n",
       "           -0.1768],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]]]], dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)), (tensor([[[[ 6.9336e-02,  6.6895e-02, -2.1118e-02,  8.8379e-02, -3.8086e-02,\n",
       "            5.8594e-02, -1.4062e-01, -1.8555e-02],\n",
       "          [-5.6152e-02,  1.3281e-01, -2.0605e-01, -4.0283e-02,  3.7354e-02,\n",
       "            3.1445e-01, -6.6406e-02,  2.7313e-03],\n",
       "          [ 1.8677e-02, -2.2339e-02, -3.9551e-02, -1.3379e-01, -9.4727e-02,\n",
       "            8.3008e-02, -2.0264e-02, -1.6357e-02],\n",
       "          [-7.7637e-02, -1.0352e-01, -6.3477e-02,  7.9590e-02, -1.4648e-02,\n",
       "           -9.2773e-02, -1.3184e-01, -2.5146e-02],\n",
       "          [-1.3574e-01,  1.4355e-01, -2.5781e-01, -1.4453e-01,  7.5195e-02,\n",
       "            2.2168e-01,  7.7637e-02,  9.0332e-02],\n",
       "          [ 1.8262e-01,  9.6680e-02, -1.9238e-01, -1.7383e-01, -1.4404e-02,\n",
       "            1.6406e-01,  9.7168e-02,  2.6953e-01],\n",
       "          [ 1.5820e-01,  2.3633e-01,  6.2988e-02, -4.1504e-02,  1.7383e-01,\n",
       "            1.4160e-02, -1.7773e-01, -2.9541e-02],\n",
       "          [ 8.5938e-02,  1.6992e-01, -1.2793e-01,  9.2773e-02,  1.6211e-01,\n",
       "            2.0703e-01,  1.2891e-01,  6.8359e-02],\n",
       "          [-8.9844e-02,  1.4844e-01, -1.2891e-01,  9.2773e-02,  1.5918e-01,\n",
       "            2.2363e-01,  1.2793e-01,  6.8848e-02],\n",
       "          [-1.8262e-01,  1.2500e-01, -1.3086e-01,  9.2773e-02,  1.0254e-02,\n",
       "            2.3730e-01,  1.2695e-01,  6.8848e-02],\n",
       "          [-1.0742e-01,  1.0156e-01, -1.3184e-01,  9.2773e-02, -1.4844e-01,\n",
       "            2.4805e-01,  1.2598e-01,  6.8848e-02],\n",
       "          [ 6.6895e-02,  7.6172e-02, -1.3281e-01,  9.2285e-02, -1.7090e-01,\n",
       "            2.5586e-01,  1.2451e-01,  6.8848e-02],\n",
       "          [ 1.7969e-01,  4.9805e-02, -1.3477e-01,  9.2285e-02, -3.5645e-02,\n",
       "            2.6367e-01,  1.2354e-01,  6.8848e-02],\n",
       "          [ 1.2793e-01,  2.2949e-02, -1.3574e-01,  9.2285e-02,  1.3184e-01,\n",
       "            2.6758e-01,  1.2207e-01,  6.8848e-02],\n",
       "          [-4.1992e-02, -3.4180e-03, -1.3672e-01,  9.2285e-02,  1.7871e-01,\n",
       "            2.6758e-01,  1.2012e-01,  6.9336e-02]],\n",
       "\n",
       "         [[-2.6489e-02, -6.5918e-02,  1.4258e-01, -1.6016e-01,  1.2598e-01,\n",
       "           -1.4551e-01, -9.0332e-02,  1.9238e-01],\n",
       "          [-7.7148e-02,  1.9434e-01, -6.3782e-03, -1.3281e-01, -1.0254e-02,\n",
       "           -2.4902e-01, -1.7969e-01,  5.2490e-02],\n",
       "          [-3.4424e-02,  7.4707e-02, -5.9814e-02, -6.5430e-02,  1.7383e-01,\n",
       "           -3.4570e-01, -1.1377e-01, -1.5625e-01],\n",
       "          [ 2.2266e-01,  1.0010e-01, -1.2793e-01, -1.8555e-01, -4.6387e-02,\n",
       "            4.1504e-02,  1.4893e-02,  1.0889e-01],\n",
       "          [-5.2734e-02, -2.0312e-01, -1.1279e-01,  1.2891e-01,  1.7773e-01,\n",
       "            2.5000e-01, -1.3184e-01,  3.8574e-02],\n",
       "          [-1.6016e-01, -1.2207e-02, -1.0596e-01,  1.6895e-01, -1.6211e-01,\n",
       "            7.4219e-02,  1.2012e-01,  1.3770e-01],\n",
       "          [-7.2754e-02,  3.0469e-01, -1.3428e-02,  3.7598e-02,  2.8442e-02,\n",
       "           -1.6699e-01,  1.2207e-03,  3.8818e-02],\n",
       "          [ 8.9844e-02, -1.2988e-01,  8.2520e-02,  1.3770e-01, -1.8945e-01,\n",
       "           -1.5137e-01,  5.8594e-03,  7.3242e-02],\n",
       "          [ 2.0898e-01, -1.1377e-01,  8.2520e-02,  1.3770e-01, -2.6733e-02,\n",
       "           -1.6309e-01,  6.7139e-03,  7.3242e-02],\n",
       "          [ 1.3477e-01, -9.7168e-02,  8.2520e-02,  1.3770e-01,  1.6016e-01,\n",
       "           -1.7383e-01,  7.5073e-03,  7.3730e-02],\n",
       "          [-6.2012e-02, -7.9102e-02,  8.2520e-02,  1.3770e-01,  2.0117e-01,\n",
       "           -1.8164e-01,  8.3008e-03,  7.3730e-02],\n",
       "          [-2.0215e-01, -6.0547e-02,  8.2520e-02,  1.3770e-01,  5.5908e-02,\n",
       "           -1.8945e-01,  9.1553e-03,  7.3730e-02],\n",
       "          [-1.5625e-01, -4.1504e-02,  8.2520e-02,  1.3770e-01, -1.4062e-01,\n",
       "           -1.9531e-01,  1.0010e-02,  7.3730e-02],\n",
       "          [ 3.3447e-02, -2.1851e-02,  8.2520e-02,  1.3770e-01, -2.0703e-01,\n",
       "           -1.9824e-01,  1.0864e-02,  7.4219e-02],\n",
       "          [ 1.9238e-01, -2.1973e-03,  8.2031e-02,  1.3770e-01, -8.3984e-02,\n",
       "           -1.9922e-01,  1.1658e-02,  7.4219e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.9336e-02,  6.6895e-02, -2.1118e-02,  8.8379e-02, -3.8086e-02,\n",
       "            5.8594e-02, -1.4062e-01, -1.8555e-02],\n",
       "          [-2.7344e-01,  1.1426e-01, -2.4414e-02,  1.8848e-01, -1.2891e-01,\n",
       "            1.4648e-01, -1.0193e-02, -1.3351e-03],\n",
       "          [-4.1016e-02,  1.6309e-01, -2.5977e-01,  1.6309e-01, -1.7578e-02,\n",
       "           -7.4219e-02,  5.8350e-02,  8.3008e-02],\n",
       "          [ 1.2012e-01, -9.1797e-02, -1.8555e-02, -2.2339e-02, -1.3379e-01,\n",
       "            2.3340e-01, -6.3477e-02, -1.2109e-01],\n",
       "          [-2.9297e-02, -4.9316e-02, -2.6172e-01,  4.1016e-02, -1.4587e-02,\n",
       "            1.4258e-01,  2.0215e-01,  6.2500e-02],\n",
       "          [ 7.4219e-02, -1.2354e-01, -5.7983e-03, -3.6865e-02,  1.8652e-01,\n",
       "            2.2266e-01, -6.7871e-02, -1.0156e-01],\n",
       "          [ 1.5918e-01, -6.5918e-02, -1.7578e-02, -1.8750e-01, -8.7891e-02,\n",
       "           -1.9727e-01, -6.2988e-02,  1.2891e-01],\n",
       "          [ 2.0020e-02,  5.5176e-02,  4.1992e-02, -6.1035e-02, -1.1658e-02,\n",
       "            2.7954e-02, -2.3438e-01, -1.5820e-01],\n",
       "          [-6.9824e-02,  1.9922e-01,  8.0566e-02,  1.9727e-01, -1.8652e-01,\n",
       "           -1.0547e-01,  5.9082e-02, -6.3965e-02],\n",
       "          [-1.4551e-01,  1.4258e-01,  1.3770e-01, -8.9355e-02,  1.2793e-01,\n",
       "           -1.6357e-02,  1.3184e-01,  2.3315e-02],\n",
       "          [-1.5503e-02,  2.1094e-01, -2.0117e-01,  1.3672e-01, -5.3711e-02,\n",
       "            9.8633e-02,  3.5889e-02,  7.1777e-02],\n",
       "          [ 1.1572e-01, -2.0508e-01,  1.5137e-02, -2.4170e-02,  1.1914e-01,\n",
       "            1.2451e-01, -6.3477e-02, -1.3281e-01],\n",
       "          [-8.8379e-02,  2.2461e-02, -1.2988e-01, -2.2754e-01,  2.6172e-01,\n",
       "            2.7734e-01,  5.3711e-02,  5.8838e-02],\n",
       "          [-7.5195e-02,  2.5781e-01, -7.1289e-02,  9.5215e-02, -2.3071e-02,\n",
       "            2.4805e-01,  1.3086e-01,  1.3086e-01],\n",
       "          [ 1.2988e-01,  1.7676e-01,  1.4453e-01,  1.3965e-01,  2.4707e-01,\n",
       "           -1.0620e-02,  1.7871e-01, -5.0293e-02]],\n",
       "\n",
       "         [[-2.6489e-02, -6.5918e-02,  1.4258e-01, -1.6016e-01,  1.2598e-01,\n",
       "           -1.4551e-01, -9.0332e-02,  1.9238e-01],\n",
       "          [ 1.6504e-01,  1.7383e-01,  4.5471e-03, -4.5166e-02, -5.1270e-02,\n",
       "           -1.5137e-01,  1.0645e-01,  2.2852e-01],\n",
       "          [ 2.6855e-03, -1.5918e-01, -7.6660e-02,  1.4453e-01, -7.5195e-02,\n",
       "            1.3770e-01, -1.8848e-01, -2.5195e-01],\n",
       "          [-4.6875e-02, -6.7383e-02,  3.5400e-02,  1.4453e-01,  5.5908e-02,\n",
       "           -3.2715e-02,  8.5938e-02,  1.0547e-01],\n",
       "          [-1.6211e-01, -1.1426e-01,  6.1768e-02,  2.3438e-01,  1.2695e-02,\n",
       "            1.1182e-01,  2.3193e-02,  1.7944e-02],\n",
       "          [-1.7944e-02, -4.4434e-02,  1.7334e-02,  1.6992e-01, -7.9590e-02,\n",
       "           -4.0771e-02,  1.0352e-01,  1.3281e-01],\n",
       "          [ 4.7607e-02, -3.4375e-01, -1.7383e-01,  1.1963e-01, -8.2031e-02,\n",
       "            4.1992e-02,  5.1514e-02, -3.8574e-02],\n",
       "          [-8.8867e-02, -2.1973e-03, -7.6172e-02, -7.2266e-02, -4.0039e-02,\n",
       "           -6.5918e-02, -1.5918e-01, -1.9165e-02],\n",
       "          [ 1.4587e-02,  1.3672e-01, -6.8848e-02, -5.8594e-02, -7.7637e-02,\n",
       "            1.0645e-01,  9.0820e-02, -1.1597e-02],\n",
       "          [ 7.6172e-02, -8.0078e-02, -4.4922e-02,  1.0156e-01,  9.7656e-04,\n",
       "           -5.4688e-02,  1.4648e-01,  1.0449e-01],\n",
       "          [ 9.5703e-02, -2.1680e-01, -9.7656e-02,  1.7773e-01,  1.3916e-02,\n",
       "           -6.7383e-02, -1.6016e-01, -2.2949e-01],\n",
       "          [-3.5889e-02, -2.8076e-02,  3.8452e-03,  1.6113e-01, -3.9795e-02,\n",
       "           -9.1797e-02,  1.0254e-01,  1.1084e-01],\n",
       "          [-6.4941e-02,  4.0625e-01, -7.1289e-02, -1.0791e-01, -7.3730e-02,\n",
       "            2.2656e-01, -6.5918e-02, -5.2246e-02],\n",
       "          [-3.1738e-02, -1.7578e-02, -1.4941e-01,  7.2266e-02, -6.4453e-02,\n",
       "           -3.7354e-02, -1.0010e-02, -8.7402e-02],\n",
       "          [-7.1289e-02, -2.6367e-02, -1.4258e-01,  3.7079e-03, -1.3574e-01,\n",
       "            1.3477e-01,  1.6504e-01, -9.6436e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 6.9336e-02,  6.6895e-02, -2.1118e-02,  8.8379e-02, -3.8086e-02,\n",
       "            5.8594e-02, -1.4062e-01, -1.8555e-02],\n",
       "          [-1.5527e-01,  9.2285e-02,  5.2979e-02, -2.5269e-02,  5.8594e-03,\n",
       "            2.5195e-01, -4.9805e-02,  6.6895e-02],\n",
       "          [-4.0771e-02, -3.0396e-02, -1.4551e-01,  7.5195e-02, -2.1606e-02,\n",
       "            3.2227e-02,  5.8350e-02,  1.7969e-01],\n",
       "          [-3.5400e-03,  1.4258e-01, -2.3242e-01,  1.4771e-02, -1.6992e-01,\n",
       "            3.1445e-01,  9.3262e-02,  1.6016e-01],\n",
       "          [ 1.4160e-01,  1.1084e-01, -2.3340e-01,  1.4648e-02, -9.4727e-02,\n",
       "            3.2617e-01,  9.0820e-02,  1.6016e-01],\n",
       "          [ 1.5625e-01,  7.7637e-02, -2.3438e-01,  1.4465e-02,  6.7383e-02,\n",
       "            3.3594e-01,  8.8867e-02,  1.6016e-01],\n",
       "          [ 2.7344e-02,  4.1992e-02, -2.3535e-01,  1.4282e-02,  1.6797e-01,\n",
       "            3.4375e-01,  8.6426e-02,  1.6016e-01],\n",
       "          [-1.2598e-01,  8.7891e-03, -2.3535e-01,  1.4160e-02,  1.1328e-01,\n",
       "            3.4375e-01,  8.3496e-02,  1.6016e-01],\n",
       "          [-1.6406e-01, -2.6367e-02, -2.3633e-01,  1.3977e-02, -4.4922e-02,\n",
       "            3.4375e-01,  8.1055e-02,  1.6016e-01],\n",
       "          [-5.1025e-02, -6.0547e-02, -2.3730e-01,  1.3794e-02, -1.6211e-01,\n",
       "            3.3984e-01,  7.9102e-02,  1.6016e-01],\n",
       "          [ 1.0938e-01, -9.3750e-02, -2.3828e-01,  1.3672e-02, -1.3086e-01,\n",
       "            3.3203e-01,  7.6660e-02,  1.6016e-01],\n",
       "          [ 1.6895e-01, -1.2598e-01, -2.3828e-01,  1.3489e-02,  2.1362e-02,\n",
       "            3.2031e-01,  7.3730e-02,  1.6016e-01],\n",
       "          [ 7.2754e-02, -1.5723e-01, -2.3926e-01,  1.3306e-02,  1.5332e-01,\n",
       "            3.0664e-01,  7.1777e-02,  1.6016e-01],\n",
       "          [-8.9355e-02, -1.8750e-01, -2.4023e-01,  1.3184e-02,  1.4453e-01,\n",
       "            2.8906e-01,  6.9336e-02,  1.6016e-01],\n",
       "          [-1.7090e-01, -2.1484e-01, -2.4023e-01,  1.3000e-02,  2.5635e-03,\n",
       "            2.6758e-01,  6.7383e-02,  1.6016e-01]],\n",
       "\n",
       "         [[-2.6489e-02, -6.5918e-02,  1.4258e-01, -1.6016e-01,  1.2598e-01,\n",
       "           -1.4551e-01, -9.0332e-02,  1.9238e-01],\n",
       "          [ 1.2305e-01,  2.5586e-01,  3.6240e-04,  1.2354e-01,  9.0332e-03,\n",
       "           -1.7578e-01, -1.2988e-01,  1.5039e-01],\n",
       "          [ 2.9053e-02, -1.1377e-01,  3.8818e-02, -3.2715e-02, -1.9287e-02,\n",
       "            8.0078e-02,  6.4453e-02,  8.3496e-02],\n",
       "          [-3.3203e-02,  7.0801e-02, -6.9824e-02,  1.4160e-01,  2.4902e-01,\n",
       "           -1.1914e-01, -6.6895e-02,  1.7383e-01],\n",
       "          [-2.2852e-01,  8.2520e-02, -6.9336e-02,  1.4160e-01,  1.0645e-01,\n",
       "           -1.1133e-01, -6.7871e-02,  1.7480e-01],\n",
       "          [-2.1191e-01,  9.2773e-02, -6.8359e-02,  1.4160e-01, -1.3379e-01,\n",
       "           -1.0303e-01, -6.8359e-02,  1.7480e-01],\n",
       "          [-1.9531e-03,  1.0303e-01, -6.7871e-02,  1.4160e-01, -2.5195e-01,\n",
       "           -9.2285e-02, -6.9336e-02,  1.7480e-01],\n",
       "          [ 2.1094e-01,  1.1182e-01, -6.6895e-02,  1.4160e-01, -1.3770e-01,\n",
       "           -8.2031e-02, -6.9336e-02,  1.7480e-01],\n",
       "          [ 2.2949e-01,  1.1914e-01, -6.5918e-02,  1.4160e-01,  1.0254e-01,\n",
       "           -7.0312e-02, -7.0312e-02,  1.7480e-01],\n",
       "          [ 3.7354e-02,  1.2598e-01, -6.5430e-02,  1.4062e-01,  2.4902e-01,\n",
       "           -5.8105e-02, -7.0801e-02,  1.7480e-01],\n",
       "          [-1.8945e-01,  1.3086e-01, -6.4941e-02,  1.4062e-01,  1.6602e-01,\n",
       "           -4.5410e-02, -7.1777e-02,  1.7480e-01],\n",
       "          [-2.4219e-01,  1.3477e-01, -6.3965e-02,  1.4062e-01, -6.9336e-02,\n",
       "           -3.2227e-02, -7.2266e-02,  1.7578e-01],\n",
       "          [-7.2266e-02,  1.3770e-01, -6.3477e-02,  1.4062e-01, -2.4023e-01,\n",
       "           -1.8799e-02, -7.3242e-02,  1.7578e-01],\n",
       "          [ 1.6406e-01,  1.3867e-01, -6.2988e-02,  1.4062e-01, -1.9141e-01,\n",
       "           -4.8828e-03, -7.3730e-02,  1.7578e-01],\n",
       "          [ 2.5000e-01,  1.3867e-01, -6.1768e-02,  1.4062e-01,  3.4668e-02,\n",
       "            9.0332e-03, -7.4219e-02,  1.7578e-01]]]], dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-0.0771,  0.0820,  0.2578,  0.0845, -0.0986,  0.0260,  0.2285,\n",
       "           -0.0723],\n",
       "          [-0.1196,  0.0442,  0.2539,  0.1289,  0.0894,  0.0050,  0.1245,\n",
       "           -0.1377],\n",
       "          [-0.1011, -0.0381,  0.1943, -0.0703, -0.2305, -0.1064,  0.0116,\n",
       "            0.0786],\n",
       "          [ 0.0400, -0.0189,  0.1426,  0.1123,  0.0630,  0.1221,  0.1426,\n",
       "           -0.1196],\n",
       "          [ 0.0317, -0.0894,  0.0464, -0.0393, -0.0608, -0.1582,  0.1187,\n",
       "           -0.0245],\n",
       "          [-0.1060, -0.0148, -0.0806, -0.0801, -0.0166,  0.0298,  0.1172,\n",
       "            0.1035],\n",
       "          [-0.2139,  0.1670,  0.1177, -0.1504,  0.0459, -0.0344,  0.0250,\n",
       "           -0.2080],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240],\n",
       "          [ 0.0981,  0.1167,  0.0815,  0.1934,  0.1924, -0.0074, -0.0938,\n",
       "           -0.0240]],\n",
       "\n",
       "         [[-0.0405,  0.1777, -0.0500, -0.0074,  0.0311,  0.0289,  0.0723,\n",
       "           -0.1230],\n",
       "          [ 0.0625, -0.1348, -0.0698, -0.1123, -0.1924, -0.1226,  0.0918,\n",
       "            0.0806],\n",
       "          [ 0.1787, -0.0200,  0.0630, -0.1064, -0.0879, -0.0771,  0.0437,\n",
       "           -0.0530],\n",
       "          [ 0.0427, -0.1152, -0.1416,  0.0845,  0.0491, -0.0708,  0.2441,\n",
       "           -0.0752],\n",
       "          [ 0.0771, -0.1523, -0.0017, -0.0122,  0.0038, -0.0986, -0.0962,\n",
       "           -0.1455],\n",
       "          [ 0.1221,  0.1523, -0.0449, -0.0076,  0.0854,  0.0649,  0.1631,\n",
       "           -0.2500],\n",
       "          [-0.0481, -0.0801, -0.0255, -0.1748, -0.0417, -0.0131, -0.1338,\n",
       "            0.0139],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437],\n",
       "          [ 0.0308,  0.2793, -0.0928, -0.1187,  0.0016,  0.0039,  0.0098,\n",
       "           -0.0437]]],\n",
       "\n",
       "\n",
       "        [[[-0.0771,  0.0820,  0.2578,  0.0845, -0.0986,  0.0260,  0.2285,\n",
       "           -0.0723],\n",
       "          [ 0.0059,  0.0415,  0.1836, -0.1953,  0.1611,  0.1143,  0.0126,\n",
       "           -0.0398],\n",
       "          [-0.0752,  0.0117,  0.0120, -0.0413,  0.1226, -0.0757,  0.0347,\n",
       "            0.1235],\n",
       "          [ 0.0520, -0.0064,  0.0219, -0.1650, -0.2227, -0.0732, -0.0280,\n",
       "           -0.1797],\n",
       "          [ 0.0364,  0.0182, -0.0449,  0.0771, -0.2402, -0.1128, -0.0063,\n",
       "            0.2520],\n",
       "          [ 0.0393, -0.0004,  0.0201, -0.1680, -0.2432, -0.0889, -0.0248,\n",
       "           -0.1602],\n",
       "          [-0.0219, -0.1436, -0.0923, -0.0292, -0.1348, -0.1670,  0.1475,\n",
       "            0.1094],\n",
       "          [ 0.0923,  0.1641,  0.1611,  0.0058,  0.1226,  0.0603,  0.0732,\n",
       "           -0.0786],\n",
       "          [-0.0134, -0.0522, -0.0981,  0.0845, -0.1289, -0.1982, -0.1738,\n",
       "            0.0991],\n",
       "          [-0.1533, -0.1865, -0.2139, -0.0981,  0.0366, -0.1865, -0.0094,\n",
       "            0.0850],\n",
       "          [-0.0747,  0.0123,  0.0129, -0.0245,  0.1182, -0.1089,  0.0315,\n",
       "            0.1216],\n",
       "          [ 0.0583, -0.0011,  0.0115, -0.1592, -0.2158, -0.0908, -0.0288,\n",
       "           -0.1797],\n",
       "          [-0.0009,  0.0723,  0.0540,  0.0208,  0.1367,  0.0055, -0.1045,\n",
       "            0.0796],\n",
       "          [-0.0199,  0.0413, -0.0537,  0.0620,  0.1895, -0.0669, -0.1104,\n",
       "            0.1279],\n",
       "          [ 0.1787,  0.1426, -0.2236,  0.1050,  0.2051,  0.0181, -0.0139,\n",
       "           -0.0161]],\n",
       "\n",
       "         [[-0.0405,  0.1777, -0.0500, -0.0074,  0.0311,  0.0289,  0.0723,\n",
       "           -0.1230],\n",
       "          [-0.0023,  0.1719,  0.0452,  0.2041, -0.0164,  0.0918,  0.2637,\n",
       "           -0.1147],\n",
       "          [ 0.0547, -0.1030, -0.0645, -0.1768, -0.1387, -0.0183, -0.0022,\n",
       "           -0.1079],\n",
       "          [-0.1025, -0.1426,  0.0625, -0.1089, -0.0217,  0.0026,  0.0107,\n",
       "            0.0613],\n",
       "          [ 0.0874,  0.0493,  0.1582, -0.0623,  0.0527,  0.0420, -0.0605,\n",
       "           -0.1006],\n",
       "          [-0.0884, -0.1387,  0.0962, -0.1074, -0.0173,  0.0214,  0.0101,\n",
       "            0.0618],\n",
       "          [ 0.1523, -0.1504, -0.0437, -0.0079,  0.0957,  0.0684,  0.0403,\n",
       "           -0.1504],\n",
       "          [-0.1001, -0.0825,  0.1348,  0.0505, -0.1230, -0.0289, -0.1699,\n",
       "            0.0518],\n",
       "          [ 0.0099, -0.1235,  0.0408,  0.0086, -0.1309, -0.0262, -0.0035,\n",
       "            0.1973],\n",
       "          [ 0.0859, -0.0178, -0.0530, -0.0869,  0.0090,  0.1797, -0.0952,\n",
       "            0.0742],\n",
       "          [ 0.0752, -0.0957, -0.0603, -0.1914, -0.1426,  0.0068, -0.0610,\n",
       "           -0.0583],\n",
       "          [-0.0972, -0.1445,  0.0762, -0.1172, -0.0258,  0.0221, -0.0238,\n",
       "            0.0845],\n",
       "          [ 0.1963, -0.0145,  0.1060,  0.0757, -0.1562, -0.0237, -0.0635,\n",
       "            0.0845],\n",
       "          [ 0.1079,  0.1621, -0.0962, -0.0090, -0.1553,  0.0229,  0.0952,\n",
       "            0.0312],\n",
       "          [ 0.0044, -0.0811,  0.0435,  0.1406,  0.2031,  0.2021, -0.1357,\n",
       "            0.0228]]],\n",
       "\n",
       "\n",
       "        [[[-0.0771,  0.0820,  0.2578,  0.0845, -0.0986,  0.0260,  0.2285,\n",
       "           -0.0723],\n",
       "          [-0.0527,  0.0481,  0.1465, -0.0952,  0.1235,  0.2314, -0.0201,\n",
       "           -0.0830],\n",
       "          [ 0.0109, -0.0464,  0.1006,  0.0544, -0.1787,  0.0344,  0.2295,\n",
       "           -0.0479],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354],\n",
       "          [-0.0908,  0.1455,  0.2891,  0.0444, -0.0645,  0.0317,  0.1494,\n",
       "           -0.0354]],\n",
       "\n",
       "         [[-0.0405,  0.1777, -0.0500, -0.0074,  0.0311,  0.0289,  0.0723,\n",
       "           -0.1230],\n",
       "          [-0.2080,  0.0957,  0.0339, -0.0374, -0.1152,  0.0439,  0.0977,\n",
       "           -0.0308],\n",
       "          [ 0.0249,  0.0203, -0.0581,  0.0141,  0.0723, -0.0239,  0.0674,\n",
       "           -0.2402],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963],\n",
       "          [ 0.0815,  0.1133, -0.0669, -0.0830, -0.0554, -0.0195,  0.2256,\n",
       "           -0.1963]]]], dtype=torch.bfloat16, grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
