{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/Tools/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, OrderedDict, Optional, List, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "\n",
    "import regex as re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import MistralConfig, MistralForCausalLM, LlamaTokenizer\n",
    "from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformers.utils.hub import cached_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clay/Tools/anaconda3/envs/llms/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"echarlaix/tiny-random-mistral\"\n",
    "\n",
    "config = MistralConfig.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "model = MistralForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 32, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x MistralDecoderLayer(\n",
       "        (self_attn): MistralFlashAttention2(\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=16, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=16, bias=False)\n",
       "          (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=32, out_features=37, bias=False)\n",
       "          (up_proj): Linear(in_features=32, out_features=37, bias=False)\n",
       "          (down_proj): Linear(in_features=37, out_features=32, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 32,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 37,\n",
       "  \"is_decoder\": true,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"type_vocab_size\": 16,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MistralAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MISTRAL_ATTENTION_CLASSES \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mMistralAttention\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: MistralAttention2,\n\u001b[1;32m      4\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MistralAttention' is not defined"
     ]
    }
   ],
   "source": [
    "MISTRAL_ATTENTION_CLASSES = {\n",
    "    \"eager\": MistralAttention,\n",
    "    \"flash_attention_2\": MistralFlashAttention2,\n",
    "    \"sdpa\": MistralSdpaAttention\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(\n",
    "    key_states: torch.Tensor,\n",
    "    value_states: torch.Tensor,\n",
    "    num_kv_groups: int,\n",
    ") -> torch.Tensor:\n",
    "    batch_size, num_heads, seq_len, head_size = key_states.size()\n",
    "    key_states = key_states[:, :, None, :, :].expand(\n",
    "        batch_size,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        seq_len,\n",
    "        head_size,\n",
    "    )\n",
    "\n",
    "    value_states = value_states[:, :, None, :, :].expand(\n",
    "        batch_size,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        seq_len,\n",
    "        head_size,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        key_states.reshape(batch_size, num_heads * num_kv_groups, seq_len, head_size),\n",
    "        value_states.reshape(batch_size, num_heads * num_kv_groups, seq_len, head_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    \"\"\"Base, abstract class for all caches.\"\"\"\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return the updated key and value states.\"\"\"\n",
    "        raise NotImplementedError(\"Make sure to implement `update` method in a subclass.\")\n",
    "    \n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        raise NotImplementedError(\"Make sure to implement `get_seq_length` in subclass.\")\n",
    "    \n",
    "    def get_max_length(self) -> int:\n",
    "        raise NotImplementedError(\"Make sure to implement `get_max_length` in subclass.\")\n",
    "    \n",
    "    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n",
    "        max_length = self.get_max_length()\n",
    "        previous_seq_length = self.get_seq_length(layer_idx=layer_idx)\n",
    "\n",
    "        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n",
    "            return max_length - new_seq_length\n",
    "        \n",
    "        return previous_seq_length\n",
    "    \n",
    "\n",
    "class DynamicCache(Cache):\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        self.seen_tokens = 0  # Used in `generate`: how many tokens the cache has seen\n",
    "\n",
    "    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n",
    "        if layer_idx < len(self):\n",
    "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "        \n",
    "        raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for layer_idx in range(len(self)):\n",
    "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_cache)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if layer_idx == 0:\n",
    "            self.seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        # Update the cache\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        else:\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "    \n",
    "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            return 0\n",
    "        \n",
    "        return self.key_cache[layer_idx].shape[-2]\n",
    "    \n",
    "    def get_max_length(self) -> Optional[int]:\n",
    "        return None\n",
    "    \n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n",
    "        for layer_idx in range(len(self.key_cache)):\n",
    "            device = self.key_cache[layer_idx].device\n",
    "            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "\n",
    "            device = self.value_cache[layer_idx].device\n",
    "            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
    "\n",
    "    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        legacy_cache = ()\n",
    "        for layer_idx in range(len(self)):\n",
    "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
    "\n",
    "        return legacy_cache\n",
    "    \n",
    "    @classmethod\n",
    "    def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n",
    "        cache = cls()\n",
    "        if past_key_values is not None:\n",
    "            for layer_idx in range(len(past_key_values)):\n",
    "                key_states, value_states = past_key_values[layer_idx]\n",
    "                cache.update(\n",
    "                    key_states=key_states,\n",
    "                    value_states=value_states,\n",
    "                    layer_idx=layer_idx,\n",
    "                )\n",
    "        return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralRMSNorm\n",
    "$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}} \\newline$\n",
    "$\\widehat{x} = g\\frac{x_i}{\\sigma}+b$\n",
    "\n",
    "But mistral does not use `bias` in RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRMSNorm(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps=1e-6) -> None:\n",
    "        \"\"\"\n",
    "        The RMSNorm is implemented according `modeling_mistral.py`.\n",
    "        It is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        original_input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "        return self.weight * hidden_states.to(original_input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralRotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_size: int,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        base: int = 10000,\n",
    "        device: Optional[Union[torch.device, str]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.theta = 1 / (base ** (torch.arange(0, head_size, 2).float() / head_size))\n",
    "        self.theta = torch.cat([self.theta, self.theta], dim=-1).to(device)\n",
    "        self.position_ids = torch.arange(0, max_position_embeddings).to(device)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, position_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        position_maxtrix = torch.outer(self.position_ids, self.theta)\n",
    "        cos = torch.cos(position_maxtrix)\n",
    "        sin = torch.sin(position_maxtrix)\n",
    "\n",
    "        x1 = hidden_states[..., :hidden_states.shape[-1] // 2]\n",
    "        x2 = hidden_states[..., hidden_states.shape[-1] // 2 :]\n",
    "        _x = torch.cat([-x2, x1], dim=-1)\n",
    "        out = hidden_states * cos[position_ids] + _x * sin[position_ids]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_heads = 2\n",
    "seq_len = 100\n",
    "head_size = 32\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_len, head_size)\n",
    "key = torch.rand(batch_size, num_heads, seq_len, head_size)\n",
    "value = torch.rand(batch_size, num_heads, seq_len, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4986, 0.4978, 0.5310,  ..., 0.4961, 0.5031, 0.5094],\n",
       "          [0.4945, 0.5042, 0.5389,  ..., 0.4876, 0.5009, 0.5086],\n",
       "          [0.4953, 0.5013, 0.5344,  ..., 0.4923, 0.4959, 0.5067],\n",
       "          ...,\n",
       "          [0.4956, 0.5026, 0.5377,  ..., 0.4932, 0.4978, 0.5067],\n",
       "          [0.4969, 0.4958, 0.5298,  ..., 0.4919, 0.5036, 0.5153],\n",
       "          [0.4948, 0.5036, 0.5320,  ..., 0.4947, 0.4992, 0.5061]],\n",
       "\n",
       "         [[0.5094, 0.4495, 0.5039,  ..., 0.4583, 0.5013, 0.5393],\n",
       "          [0.5086, 0.4448, 0.5077,  ..., 0.4569, 0.5004, 0.5357],\n",
       "          [0.5084, 0.4494, 0.5007,  ..., 0.4601, 0.5049, 0.5335],\n",
       "          ...,\n",
       "          [0.5111, 0.4432, 0.5004,  ..., 0.4572, 0.5039, 0.5322],\n",
       "          [0.5077, 0.4446, 0.4999,  ..., 0.4566, 0.5044, 0.5335],\n",
       "          [0.5064, 0.4423, 0.5060,  ..., 0.4557, 0.5018, 0.5348]]],\n",
       "\n",
       "\n",
       "        [[[0.5381, 0.5128, 0.5104,  ..., 0.5144, 0.5449, 0.5217],\n",
       "          [0.5419, 0.5128, 0.5113,  ..., 0.5127, 0.5414, 0.5202],\n",
       "          [0.5393, 0.5156, 0.5109,  ..., 0.5129, 0.5385, 0.5227],\n",
       "          ...,\n",
       "          [0.5394, 0.5164, 0.5112,  ..., 0.5117, 0.5461, 0.5193],\n",
       "          [0.5361, 0.5159, 0.5100,  ..., 0.5100, 0.5450, 0.5239],\n",
       "          [0.5384, 0.5150, 0.5136,  ..., 0.5136, 0.5401, 0.5241]],\n",
       "\n",
       "         [[0.4953, 0.4972, 0.4713,  ..., 0.5223, 0.5444, 0.4567],\n",
       "          [0.4969, 0.4957, 0.4729,  ..., 0.5250, 0.5400, 0.4564],\n",
       "          [0.4934, 0.4977, 0.4715,  ..., 0.5229, 0.5453, 0.4597],\n",
       "          ...,\n",
       "          [0.4944, 0.4893, 0.4669,  ..., 0.5300, 0.5405, 0.4555],\n",
       "          [0.4890, 0.4931, 0.4698,  ..., 0.5220, 0.5425, 0.4562],\n",
       "          [0.4955, 0.4969, 0.4707,  ..., 0.5273, 0.5457, 0.4571]]],\n",
       "\n",
       "\n",
       "        [[[0.5581, 0.4980, 0.4639,  ..., 0.4826, 0.4949, 0.5371],\n",
       "          [0.5575, 0.4923, 0.4647,  ..., 0.4880, 0.4942, 0.5318],\n",
       "          [0.5566, 0.4927, 0.4646,  ..., 0.4862, 0.4944, 0.5314],\n",
       "          ...,\n",
       "          [0.5565, 0.4922, 0.4680,  ..., 0.4899, 0.4975, 0.5300],\n",
       "          [0.5599, 0.4897, 0.4648,  ..., 0.4918, 0.4972, 0.5339],\n",
       "          [0.5592, 0.4950, 0.4661,  ..., 0.4833, 0.4889, 0.5300]],\n",
       "\n",
       "         [[0.5484, 0.4760, 0.4897,  ..., 0.4985, 0.4651, 0.4862],\n",
       "          [0.5464, 0.4797, 0.4928,  ..., 0.4955, 0.4573, 0.4897],\n",
       "          [0.5417, 0.4784, 0.4938,  ..., 0.4941, 0.4532, 0.4898],\n",
       "          ...,\n",
       "          [0.5432, 0.4799, 0.4906,  ..., 0.4969, 0.4534, 0.4869],\n",
       "          [0.5482, 0.4812, 0.4931,  ..., 0.4998, 0.4605, 0.4880],\n",
       "          [0.5436, 0.4757, 0.4903,  ..., 0.4970, 0.4603, 0.4867]]],\n",
       "\n",
       "\n",
       "        [[[0.4670, 0.4602, 0.5202,  ..., 0.5439, 0.5039, 0.5160],\n",
       "          [0.4645, 0.4639, 0.5202,  ..., 0.5420, 0.5041, 0.5149],\n",
       "          [0.4673, 0.4633, 0.5210,  ..., 0.5446, 0.5024, 0.5132],\n",
       "          ...,\n",
       "          [0.4684, 0.4687, 0.5192,  ..., 0.5398, 0.4990, 0.5135],\n",
       "          [0.4632, 0.4580, 0.5219,  ..., 0.5425, 0.5033, 0.5102],\n",
       "          [0.4673, 0.4609, 0.5223,  ..., 0.5428, 0.5008, 0.5117]],\n",
       "\n",
       "         [[0.4945, 0.5524, 0.4537,  ..., 0.5109, 0.5118, 0.5031],\n",
       "          [0.4922, 0.5534, 0.4516,  ..., 0.5097, 0.5130, 0.5021],\n",
       "          [0.4866, 0.5504, 0.4531,  ..., 0.5081, 0.5094, 0.5062],\n",
       "          ...,\n",
       "          [0.4883, 0.5503, 0.4511,  ..., 0.5129, 0.5148, 0.5078],\n",
       "          [0.4933, 0.5493, 0.4474,  ..., 0.5128, 0.5145, 0.5061],\n",
       "          [0.4884, 0.5599, 0.4531,  ..., 0.5078, 0.5100, 0.5044]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdpa_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query=query,\n",
    "    key=key,\n",
    "    value=value,\n",
    ")\n",
    "sdpa_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4986, 0.4978, 0.5310,  ..., 0.4961, 0.5031, 0.5094],\n",
       "          [0.4945, 0.5042, 0.5389,  ..., 0.4876, 0.5009, 0.5086],\n",
       "          [0.4953, 0.5013, 0.5344,  ..., 0.4923, 0.4959, 0.5067],\n",
       "          ...,\n",
       "          [0.4956, 0.5026, 0.5377,  ..., 0.4932, 0.4978, 0.5067],\n",
       "          [0.4969, 0.4958, 0.5298,  ..., 0.4919, 0.5036, 0.5153],\n",
       "          [0.4948, 0.5036, 0.5320,  ..., 0.4947, 0.4992, 0.5061]],\n",
       "\n",
       "         [[0.5094, 0.4495, 0.5039,  ..., 0.4583, 0.5013, 0.5393],\n",
       "          [0.5086, 0.4448, 0.5077,  ..., 0.4569, 0.5004, 0.5357],\n",
       "          [0.5084, 0.4494, 0.5007,  ..., 0.4601, 0.5049, 0.5335],\n",
       "          ...,\n",
       "          [0.5111, 0.4432, 0.5004,  ..., 0.4572, 0.5039, 0.5322],\n",
       "          [0.5077, 0.4446, 0.4999,  ..., 0.4566, 0.5044, 0.5335],\n",
       "          [0.5064, 0.4423, 0.5060,  ..., 0.4557, 0.5018, 0.5348]]],\n",
       "\n",
       "\n",
       "        [[[0.5381, 0.5128, 0.5104,  ..., 0.5144, 0.5449, 0.5217],\n",
       "          [0.5419, 0.5128, 0.5113,  ..., 0.5127, 0.5414, 0.5202],\n",
       "          [0.5393, 0.5156, 0.5109,  ..., 0.5129, 0.5385, 0.5227],\n",
       "          ...,\n",
       "          [0.5394, 0.5164, 0.5112,  ..., 0.5117, 0.5461, 0.5193],\n",
       "          [0.5361, 0.5159, 0.5100,  ..., 0.5100, 0.5450, 0.5239],\n",
       "          [0.5384, 0.5150, 0.5136,  ..., 0.5136, 0.5401, 0.5241]],\n",
       "\n",
       "         [[0.4953, 0.4972, 0.4713,  ..., 0.5223, 0.5444, 0.4567],\n",
       "          [0.4969, 0.4957, 0.4729,  ..., 0.5250, 0.5400, 0.4564],\n",
       "          [0.4934, 0.4977, 0.4715,  ..., 0.5229, 0.5453, 0.4597],\n",
       "          ...,\n",
       "          [0.4944, 0.4893, 0.4669,  ..., 0.5300, 0.5405, 0.4555],\n",
       "          [0.4890, 0.4931, 0.4698,  ..., 0.5220, 0.5425, 0.4562],\n",
       "          [0.4955, 0.4969, 0.4707,  ..., 0.5273, 0.5457, 0.4571]]],\n",
       "\n",
       "\n",
       "        [[[0.5581, 0.4980, 0.4639,  ..., 0.4826, 0.4949, 0.5371],\n",
       "          [0.5575, 0.4923, 0.4647,  ..., 0.4880, 0.4942, 0.5318],\n",
       "          [0.5566, 0.4927, 0.4646,  ..., 0.4862, 0.4944, 0.5314],\n",
       "          ...,\n",
       "          [0.5565, 0.4922, 0.4680,  ..., 0.4899, 0.4975, 0.5300],\n",
       "          [0.5599, 0.4897, 0.4648,  ..., 0.4918, 0.4972, 0.5339],\n",
       "          [0.5592, 0.4950, 0.4661,  ..., 0.4833, 0.4889, 0.5300]],\n",
       "\n",
       "         [[0.5484, 0.4760, 0.4897,  ..., 0.4985, 0.4651, 0.4862],\n",
       "          [0.5464, 0.4797, 0.4928,  ..., 0.4955, 0.4573, 0.4897],\n",
       "          [0.5417, 0.4784, 0.4938,  ..., 0.4941, 0.4532, 0.4898],\n",
       "          ...,\n",
       "          [0.5432, 0.4799, 0.4906,  ..., 0.4969, 0.4534, 0.4869],\n",
       "          [0.5482, 0.4812, 0.4931,  ..., 0.4998, 0.4605, 0.4880],\n",
       "          [0.5436, 0.4757, 0.4903,  ..., 0.4970, 0.4603, 0.4867]]],\n",
       "\n",
       "\n",
       "        [[[0.4670, 0.4602, 0.5202,  ..., 0.5439, 0.5039, 0.5160],\n",
       "          [0.4645, 0.4639, 0.5202,  ..., 0.5420, 0.5041, 0.5149],\n",
       "          [0.4673, 0.4633, 0.5210,  ..., 0.5446, 0.5024, 0.5132],\n",
       "          ...,\n",
       "          [0.4684, 0.4687, 0.5192,  ..., 0.5398, 0.4990, 0.5135],\n",
       "          [0.4632, 0.4580, 0.5219,  ..., 0.5425, 0.5033, 0.5102],\n",
       "          [0.4673, 0.4609, 0.5223,  ..., 0.5428, 0.5008, 0.5117]],\n",
       "\n",
       "         [[0.4945, 0.5524, 0.4537,  ..., 0.5109, 0.5118, 0.5031],\n",
       "          [0.4922, 0.5534, 0.4516,  ..., 0.5097, 0.5130, 0.5021],\n",
       "          [0.4866, 0.5504, 0.4531,  ..., 0.5081, 0.5094, 0.5062],\n",
       "          ...,\n",
       "          [0.4883, 0.5503, 0.4511,  ..., 0.5129, 0.5148, 0.5078],\n",
       "          [0.4933, 0.5493, 0.4474,  ..., 0.5128, 0.5145, 0.5061],\n",
       "          [0.4884, 0.5599, 0.4531,  ..., 0.5078, 0.5100, 0.5044]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_spda(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "    # Attention weights (Q * K^T)\n",
    "    attention_weights = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(head_size)\n",
    "\n",
    "    # Upcast attention to fp32\n",
    "    attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attention_weights = torch.nn.functional.dropout(attention_weights, p=0.1, training=False)\n",
    "\n",
    "    # Attention output (A = Q * K^T, A * V)\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return attention_output\n",
    "\n",
    "\n",
    "my_spda_output = my_spda(query=query, key=key, value=value)\n",
    "my_spda_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Implementation: <torch.utils.benchmark.utils.common.Measurement object at 0x7fd5b23f88b0>\n",
      "benchmark_my_sdpa()\n",
      "setup: from __main__ import benchmark_my_sdpa\n",
      "  131.17 us\n",
      "  1 measurement, 100000 runs , 8 threads\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "import math\n",
    "\n",
    "\n",
    "def my_sdpa(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, head_size: float) -> torch.Tensor:\n",
    "    # Attention weights (Q * K^T)\n",
    "    attention_weights = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(head_size)\n",
    "\n",
    "    # Upcast attention to fp32\n",
    "    attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attention_weights = torch.nn.functional.dropout(attention_weights, p=0.1, training=False)\n",
    "\n",
    "    # Attention output (A = Q * K^T, A * V)\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return attention_output\n",
    "\n",
    "\n",
    "def benchmark_my_sdpa():\n",
    "    return my_sdpa(query, key, value, head_size)\n",
    "\n",
    "# Testing\n",
    "t = benchmark.Timer(\n",
    "    stmt=\"benchmark_my_sdpa()\",\n",
    "    setup=\"from __main__ import benchmark_my_sdpa\",\n",
    "    globals=globals(),\n",
    "    num_threads=torch.get_num_threads(),\n",
    ")\n",
    "\n",
    "# Result\n",
    "print(\"My Implementation:\", t.timeit(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Implementation: <torch.utils.benchmark.utils.common.Measurement object at 0x7fd5b17d5a20>\n",
      "benchmark_torch_sdpa()\n",
      "setup: from __main__ import benchmark_torch_sdpa\n",
      "  90.94 us\n",
      "  1 measurement, 100000 runs , 8 threads\n"
     ]
    }
   ],
   "source": [
    "def torch_sdpa(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.nn.functional.scaled_dot_product_attention(\n",
    "        query=query,\n",
    "        key=key,\n",
    "        value=value,\n",
    "    )\n",
    "\n",
    "\n",
    "def benchmark_torch_sdpa():\n",
    "    return torch_sdpa(query, key, value)\n",
    "\n",
    "\n",
    "# Testing\n",
    "t = benchmark.Timer(\n",
    "    stmt=\"benchmark_torch_sdpa()\",\n",
    "    setup=\"from __main__ import benchmark_torch_sdpa\",\n",
    "    globals=globals(),\n",
    "    num_threads=torch.get_num_threads(),\n",
    ")\n",
    "\n",
    "# Result\n",
    "print(\"Torch Implementation:\", t.timeit(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(sdpa_output, my_spda_output, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralAttention(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Init\n",
    "        self.num_q_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.num_kv_groups = self.num_q_heads // self.num_kv_heads\n",
    "        self.head_size = config.hidden_size // self.num_q_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        # QKVO Layer\n",
    "        self.q_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.k_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.num_kv_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.v_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.num_kv_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.o_proj = torch.nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # RoPE\n",
    "        self.rotary_emb = MistralRotaryEmbedding(\n",
    "            head_size=self.head_size,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            base=config.rope_theta,\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        # Init\n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "\n",
    "        # QKV\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_q_heads, self.head_size).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_kv_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # KV Cache\n",
    "        kv_seq_len = key_states.size(2)\n",
    "        if past_key_value is not None and self.layer_idx is not None:\n",
    "            kv_seq_len += past_key_value.get_usable_length(\n",
    "                new_seq_length=kv_seq_len,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "\n",
    "        query_states = self.rotary_emb(\n",
    "            hidden_states=query_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        key_states = self.rotary_emb(\n",
    "            hidden_states=key_states,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            key_states, value_states = past_key_value.update(\n",
    "                key_states=key_states,\n",
    "                value_states=value_states,\n",
    "                layer_idx=self.layer_idx,\n",
    "            )\n",
    "        \n",
    "        # Repeat kv heads\n",
    "        key_states, value_states = repeat_kv(\n",
    "            key_states=key_states,\n",
    "            value_states=value_states,\n",
    "            num_kv_groups=self.num_kv_groups,\n",
    "        )\n",
    "\n",
    "        # Attention weights (Q * K^T)\n",
    "        attention_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
    "\n",
    "        # Attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_weights = attention_weights + attention_mask\n",
    "\n",
    "        # Upcast attention to fp32\n",
    "        attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attention_weights = torch.nn.functional.dropout(attention_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        # Attention output (A = Q * K^T, A * V)\n",
    "        attention_output = torch.matmul(attention_weights, value_states).reshape(batch_size, seq_len, self.hidden_size)\n",
    "        attention_output = self.o_proj(attention_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attention_weights = None\n",
    "\n",
    "        return attention_output, attention_weights, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralSdpaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralSdpaAttention(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        \"\"\"\n",
    "        The mutliple query heads will using the same kv head.\n",
    "        So `num_attention_heads` > `num_key_value_heads`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Init\n",
    "        self.head_size = config.hidden_size // config.num_attention_heads\n",
    "\n",
    "        # QKVO Layer\n",
    "        self.q_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.k_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.num_key_value_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.v_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.num_key_value_heads * self.head_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.o_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.hidden_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # RoPE\n",
    "        self.rotary_emb = MistralRotaryEmbedding(\n",
    "            head_size=self.head_size,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            base=config.rope_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = MistralSdpaAttention(config=config)\n",
    "        self.mlp = MistralMLP(config=config)\n",
    "        self.input_layernorm = MistralRMSNorm(config.hidden_size)\n",
    "        self.post_attention_layernorm = MistralRMSNorm(config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralModel(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = torch.nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "            padding_idx=config.pad_token_id,\n",
    "        )\n",
    "        self.layers = torch.nn.ModuleList([MistralDecoderLayer(config=config)] for _ in range(config.num_hidden_layers))\n",
    "        self.norm = MistralRMSNorm(config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMistralModelForCausalLM(torch.nn.Module):\n",
    "    def __init__(self, config: MistralConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.model = MistralModel(config=config)\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            input_features=config.hidden_size,\n",
    "            out_features=config.vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "    ) -> torch.Tensor:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
